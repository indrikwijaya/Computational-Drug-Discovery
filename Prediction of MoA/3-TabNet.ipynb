{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3-TabNet.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyO4HX5PGk776A57oOukdeYY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Tabnet\n","\n","For tabular data, the most common approach and winning method in many Kaggle competitions is tree-based models and their ensembles. XGBoost and LightGBM are two examples that have been dominating many winning solutions. In recent years, there has been an effort to develop Deep Learning algorithm for tabular data. One such successful effort is from Google Cloud AI, which is call TabNet. An important characteristic feature of this algorithm is that 'it combines the features of neural networks to fit very complex functions and the feature selection property of tree-based algorithms.' Additionally, in contrast with tree-based models that can only do feature-selection globally, TabNet allows instance-wise feature-selection. Most importanly, TabNet provides interpretability which is a key desirable feature in any machine learning algorithm. \n","\n","<figure><center>\n","<img src = https://miro.medium.com/max/700/1*twB1nZHPN5Cuxu2h_jpEPg.png>\n","<figcaption> Source: https://arxiv.org/pdf/1908.07442v1.pdf </figcaption> </center></figure>\n","\n","The TabNet implementation is largely adapted from this [notebook](https://www.kaggle.com/samratthapa/tabnet-implementation/notebook?scriptVersionId=46472520)"],"metadata":{"id":"EJWg7dvQYYg-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"jVCx9yDZPNVa"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset,DataLoader\n","from sklearn.preprocessing import QuantileTransformer\n","\n","\n","from sklearn import preprocessing\n","import torch.optim as optim"]},{"cell_type":"code","source":["import torch.nn.functional as F\n"],"metadata":{"id":"vZ5Bt4cbPVrU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install optuna\n","import optuna\n","import plotly as pl"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KsPJzOiFPXzG","executionInfo":{"status":"ok","timestamp":1642557820329,"user_tz":-480,"elapsed":8935,"user":{"displayName":"Indrik Wijaya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00208067985282844199"}},"outputId":"309a4b4f-7c72-4de7-d043-e7aad9071f96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting optuna\n","  Downloading optuna-2.10.0-py3-none-any.whl (308 kB)\n","\u001b[K     |████████████████████████████████| 308 kB 7.5 MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13)\n","Collecting colorlog\n","  Downloading colorlog-6.6.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n","Collecting cliff\n","  Downloading cliff-3.10.0-py3-none-any.whl (80 kB)\n","\u001b[K     |████████████████████████████████| 80 kB 10.3 MB/s \n","\u001b[?25hRequirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n","Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.29)\n","Collecting alembic\n","  Downloading alembic-1.7.5-py3-none-any.whl (209 kB)\n","\u001b[K     |████████████████████████████████| 209 kB 63.7 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.62.3)\n","Collecting cmaes>=0.8.2\n","  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.19.5)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.6)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.10.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n","Collecting Mako\n","  Downloading Mako-1.1.6-py2.py3-none-any.whl (75 kB)\n","\u001b[K     |████████████████████████████████| 75 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.4.0)\n","Collecting cmd2>=1.0.0\n","  Downloading cmd2-2.3.3-py3-none-any.whl (149 kB)\n","\u001b[K     |████████████████████████████████| 149 kB 64.4 MB/s \n","\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.0.0)\n","Collecting stevedore>=2.0.1\n","  Downloading stevedore-3.5.0-py3-none-any.whl (49 kB)\n","\u001b[K     |████████████████████████████████| 49 kB 6.5 MB/s \n","\u001b[?25hCollecting autopage>=0.4.0\n","  Downloading autopage-0.4.0-py3-none-any.whl (20 kB)\n","Collecting pbr!=2.1.0,>=2.0.0\n","  Downloading pbr-5.8.0-py2.py3-none-any.whl (112 kB)\n","\u001b[K     |████████████████████████████████| 112 kB 63.2 MB/s \n","\u001b[?25hRequirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (3.10.0.2)\n","Collecting pyperclip>=1.6\n","  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n","Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.7.0)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n","Building wheels for collected packages: pyperclip\n","  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=36f407fd4d355dc5372de4be23ed0345e0e65de64027835f4638ba3559bc4fb7\n","  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n","Successfully built pyperclip\n","Installing collected packages: pyperclip, pbr, stevedore, Mako, cmd2, autopage, colorlog, cmaes, cliff, alembic, optuna\n","Successfully installed Mako-1.1.6 alembic-1.7.5 autopage-0.4.0 cliff-3.10.0 cmaes-0.8.2 cmd2-2.3.3 colorlog-6.6.0 optuna-2.10.0 pbr-5.8.0 pyperclip-1.8.2 stevedore-3.5.0\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import StratifiedKFold\n"],"metadata":{"id":"sDsHUN__PZHH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import lightgbm as lgb"],"metadata":{"id":"Ev-ytH9QPeDo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n"],"metadata":{"id":"hzBR3C6IPf01"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! unzip lish-moa.zip\n","test_fea = pd.read_csv('test_features.csv')\n","train_fea = pd.read_csv('train_features.csv')\n","train_tar_nonsco = pd.read_csv('train_targets_nonscored.csv')\n","train_tar_sco = pd.read_csv('train_targets_scored.csv')\n","submission = pd.read_csv('sample_submission.csv')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c0iV_m2MPg_1","executionInfo":{"status":"ok","timestamp":1642557827192,"user_tz":-480,"elapsed":6875,"user":{"displayName":"Indrik Wijaya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00208067985282844199"}},"outputId":"44b9b819-5581-44e8-a631-e7d0c1b5c7e1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  lish-moa.zip\n","  inflating: sample_submission.csv   \n","  inflating: test_features.csv       \n","  inflating: train_drug.csv          \n","  inflating: train_features.csv      \n","  inflating: train_targets_nonscored.csv  \n","  inflating: train_targets_scored.csv  \n"]}]},{"cell_type":"code","source":["def seed_everything(seed=1062):\n","    np.random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    \n","seed_everything(seed=1062)"],"metadata":{"id":"dHhF1ThYPp2Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Sparsemax(nn.Module):\n","    def __init__(self, dim=None):\n","        super(Sparsemax, self).__init__()\n","        self.dim = -1 if dim is None else dim\n","\n","    def forward(self, input):\n","        input = input.transpose(0, self.dim)\n","        original_size = input.size()\n","        input = input.reshape(input.size(0), -1)\n","        input = input.transpose(0, 1)\n","        dim = 1\n","\n","        number_of_logits = input.size(dim)\n","        \n","        input = input - torch.max(input, dim=dim, keepdim=True)[0].expand_as(input)\n","        zs = torch.sort(input=input, dim=dim, descending=True)[0]\n","        range = torch.arange(start=1, end=number_of_logits + 1, device=device,step=1, dtype=input.dtype).view(1, -1)\n","        range = range.expand_as(zs)\n","\n","        bound = 1 + range * zs\n","        cumulative_sum_zs = torch.cumsum(zs, dim)\n","        is_gt = torch.gt(bound, cumulative_sum_zs).type(input.type())\n","        k = torch.max(is_gt * range, dim, keepdim=True)[0]\n","        zs_sparse = is_gt * zs\n","        taus = (torch.sum(zs_sparse, dim, keepdim=True) - 1) / k\n","        taus = taus.expand_as(input)\n","        self.output = torch.max(torch.zeros_like(input), input - taus)\n","        output = self.output\n","        output = output.transpose(0, 1)\n","        output = output.reshape(original_size)\n","        output = output.transpose(0, self.dim)\n","        return output\n","    def backward(self, grad_output):\n","        dim = 1\n","        nonzeros = torch.ne(self.output, 0)\n","        sum = torch.sum(grad_output * nonzeros, dim=dim) / torch.sum(nonzeros, dim=dim)\n","        self.grad_input = nonzeros * (grad_output - sum.expand_as(grad_output))\n","        return self.grad_input"],"metadata":{"id":"yZCe2sL3Ps00"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def initialize_non_glu(module,inp_dim,out_dim):\n","    gain = np.sqrt((inp_dim+out_dim)/np.sqrt(4*inp_dim))\n","    torch.nn.init.xavier_normal_(module.weight, gain=gain)\n","    \n","class GBN(nn.Module):\n","    def __init__(self,inp,vbs=128,momentum=0.01):\n","        super().__init__()\n","        self.bn = nn.BatchNorm1d(inp,momentum=momentum)\n","        self.vbs = vbs\n","    def forward(self,x):\n","        chunk = torch.chunk(x,max(1,x.size(0)//self.vbs),0)\n","        res = [self.bn(y) for y in chunk ]\n","        return torch.cat(res,0)\n","\n","class GLU(nn.Module):\n","    def __init__(self,inp_dim,out_dim,fc=None,vbs=128):\n","        super().__init__()\n","        if fc:\n","            self.fc = fc\n","        else:\n","            self.fc = nn.Linear(inp_dim,out_dim*2)\n","        self.bn = GBN(out_dim*2,vbs=vbs) \n","        self.od = out_dim\n","    def forward(self,x):\n","        x = self.bn(self.fc(x))\n","        return x[:,:self.od]*torch.sigmoid(x[:,self.od:])\n","    \n","\n","class FeatureTransformer(nn.Module):\n","    def __init__(self,inp_dim,out_dim,shared,n_ind,vbs=128):\n","        super().__init__()\n","        first = True\n","        self.shared = nn.ModuleList()\n","        if shared:\n","            self.shared.append(GLU(inp_dim,out_dim,shared[0],vbs=vbs))\n","            first= False    \n","            for fc in shared[1:]:\n","                self.shared.append(GLU(out_dim,out_dim,fc,vbs=vbs))\n","        else:\n","            self.shared = None\n","        self.independ = nn.ModuleList()\n","        if first:\n","            self.independ.append(GLU(inp,out_dim,vbs=vbs))\n","        for x in range(first, n_ind):\n","            self.independ.append(GLU(out_dim,out_dim,vbs=vbs))\n","        self.scale = torch.sqrt(torch.tensor([.5],device=device))\n","    def forward(self,x):\n","        if self.shared:\n","            x = self.shared[0](x)\n","            for glu in self.shared[1:]:\n","                x = torch.add(x, glu(x))\n","                x = x*self.scale\n","        for glu in self.independ:\n","            x = torch.add(x, glu(x))\n","            x = x*self.scale\n","        return x\n","class AttentionTransformer(nn.Module):\n","    def __init__(self,inp_dim,out_dim,relax,vbs=128):\n","        super().__init__()\n","        self.fc = nn.Linear(inp_dim,out_dim)\n","        self.bn = GBN(out_dim,vbs=vbs)\n","#         self.smax = Sparsemax()\n","        self.r = torch.tensor([relax],device=device)\n","    def forward(self,a,priors):\n","        a = self.bn(self.fc(a))\n","        mask = torch.sigmoid(a*priors)\n","        priors =priors*(self.r-mask)\n","        return mask\n","\n","class DecisionStep(nn.Module):\n","    def __init__(self,inp_dim,n_d,n_a,shared,n_ind,relax,vbs=128):\n","        super().__init__()\n","        self.fea_tran = FeatureTransformer(inp_dim,n_d+n_a,shared,n_ind,vbs)\n","        self.atten_tran = AttentionTransformer(n_a,inp_dim,relax,vbs)\n","    def forward(self,x,a,priors):\n","        mask = self.atten_tran(a,priors)\n","        loss = ((-1)*mask*torch.log(mask+1e-10)).mean()\n","        x = self.fea_tran(x*mask)\n","        return x,loss\n","\n","class TabNet(nn.Module):\n","    def __init__(self,inp_dim,final_out_dim,n_d=64,n_a=64,n_shared=2,n_ind=2,n_steps=5,relax=1.2,vbs=128):\n","        super().__init__()\n","        if n_shared>0:\n","            self.shared = nn.ModuleList()\n","            self.shared.append(nn.Linear(inp_dim,2*(n_d+n_a)))\n","            for x in range(n_shared-1):\n","                self.shared.append(nn.Linear(n_d+n_a,2*(n_d+n_a)))\n","        else:\n","            self.shared=None\n","        self.first_step = FeatureTransformer(inp_dim,n_d+n_a,self.shared,n_ind) \n","        self.steps = nn.ModuleList()\n","        for x in range(n_steps-1):\n","            self.steps.append(DecisionStep(inp_dim,n_d,n_a,self.shared,n_ind,relax,vbs))\n","        self.fc = nn.Linear(n_d,final_out_dim)\n","        self.bn = nn.BatchNorm1d(inp_dim)\n","        self.n_d = n_d\n","    def forward(self,x):\n","        x = self.bn(x)\n","        x_a = self.first_step(x)[:,self.n_d:]\n","        loss = torch.zeros(1).to(x.device)\n","        out = torch.zeros(x.size(0),self.n_d).to(x.device)\n","        priors = torch.ones(x.shape).to(x.device)\n","        for step in self.steps:\n","            x_te,l = step(x,x_a,priors)\n","            out += F.relu(x_te[:,:self.n_d])\n","            x_a = x_te[:,self.n_d:]\n","            loss += l\n","        return self.fc(out),loss"],"metadata":{"id":"shG6PpPxPvfL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TabNetWithEmbed(nn.Module):\n","    def __init__(self,inp_dim,final_out_dim,n_d=64,n_a=64,n_shared=2,n_ind=2,n_steps=5,relax=1.2,vbs=128):\n","        super().__init__()\n","        self.tabnet = TabNet(inp_dim,final_out_dim,n_d,n_a,n_shared,n_ind,n_steps,relax,vbs)\n","        self.cat_embed = []\n","        self.emb1 = nn.Embedding(2,1)\n","        self.emb3 = nn.Embedding(3,1)\n","        self.cat_embed.append(self.emb1)\n","        self.cat_embed.append(self.emb3)\n","        \n","    def forward(self,catv,contv):\n","        catv = catv.to(device)\n","        contv = contv.to(device)\n","        embeddings = [embed(catv[:,idx]) for embed,idx in zip(self.cat_embed,range(catv.size(1)))]\n","        catv = torch.cat(embeddings,1)\n","        x = torch.cat((catv,contv),1).contiguous()\n","        x,l = self.tabnet(x)\n","        return torch.sigmoid(x),l"],"metadata":{"id":"Lnv0hFIkPx3X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DrugData(Dataset):\n","    \n","    def __init__(self, df, out, train=True):\n","        self.df = df\n","        self.out = out\n","        self.train=train\n","    def __len__(self):\n","        return len(self.df)\n","    \n","    def __getitem__(self,idx):\n","        if self.train:\n","            tar = np.where(self.out[idx].reshape(-1)==0,0.00001,0.99999)\n","            return torch.from_numpy(self.df[idx,:]).float(),torch.tensor(tar).float()\n","        else:\n","            return torch.from_numpy(self.df[idx,:]).float(),torch.tensor(self.out[idx].reshape(-1)).float()"],"metadata":{"id":"H7iTPbnEPzrn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_fea['cp_dose'].replace({'D1':0,'D2':1},inplace=True)\n","test_fea['cp_dose'].replace({'D1':0,'D2':1},inplace=True)\n","\n","train_fea['cp_time'].replace({24:0,72:1,48:2},inplace=True)\n","test_fea['cp_time'].replace({24:0,72:1,48:2},inplace=True)\n","\n","train_fea['cp_type'].replace({'trt_cp':0,'ctl_vehicle':1},inplace=True)\n","test_fea['cp_type'].replace({'trt_cp':0,'ctl_vehicle':1},inplace=True)\n","\n","train_fea = train_fea.drop(columns='sig_id') \n","test_fea = test_fea.drop(columns='sig_id')"],"metadata":{"id":"HEm_TuYzP1fR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cat_col = [0,2]\n","num_col = len(train_fea.columns)"],"metadata":{"id":"eShtgHUGP3L4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_tar = train_tar_sco.drop(columns='sig_id').values"],"metadata":{"id":"alDGQXQ7P45U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_selection import VarianceThreshold\n","from sklearn.decomposition import PCA"],"metadata":{"id":"TCBqkG3EP7Ct"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = pd.concat([train_fea,test_fea],ignore_index=True)\n","g = [*(x for x in data.columns if 'g' in x)]\n","c = [*(x for x in data.columns if 'c-' in x)]"],"metadata":{"id":"1punxyE-P8S_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for col in g:\n","    sel = QuantileTransformer(n_quantiles=1000,random_state=0,output_distribution='normal')\n","    sel.fit(data[col].to_numpy().reshape(-1,1))\n","    data[col] = sel.transform(data[col].to_numpy().reshape(-1,1))\n","for col in c:\n","    sel = QuantileTransformer(n_quantiles=1000,random_state=0,output_distribution='normal')\n","    sel.fit(data[col].to_numpy().reshape(-1,1))\n","    data[col] = sel.transform(data[col].to_numpy().reshape(-1,1))"],"metadata":{"id":"_L-BA87tP9_D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pca_c = PCA(n_components=15)\n","extra_c = pd.DataFrame(pca_c.fit_transform(data[c]))\n","pca_g = PCA(n_components=50)\n","extra_g = pd.DataFrame(pca_g.fit_transform(data[g]))\n","data = pd.concat((data,extra_c,extra_g),axis=1)"],"metadata":{"id":"3qdh6FDeP_gU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install umap-learn\n","from umap import UMAP\n","\n","umap_c = UMAP(random_state=256,n_components=15)\n","extra_c = pd.DataFrame(umap_c.fit_transform(data[c]))\n","umap_g = UMAP(random_state=256,n_components=50)\n","extra_g = pd.DataFrame(umap_g.fit_transform(data[g]))\n","data = pd.concat((data,extra_c,extra_g),axis=1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Y668_jNQA8E","executionInfo":{"status":"ok","timestamp":1642558039012,"user_tz":-480,"elapsed":195429,"user":{"displayName":"Indrik Wijaya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00208067985282844199"}},"outputId":"85f30edd-e8df-44e8-99f0-7b140c9b9e9b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting umap-learn\n","  Downloading umap-learn-0.5.2.tar.gz (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 3.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.19.5)\n","Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.0.2)\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.4.1)\n","Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.51.2)\n","Collecting pynndescent>=0.5\n","  Downloading pynndescent-0.5.5.tar.gz (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 49.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn) (4.62.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.34.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn) (3.0.0)\n","Building wheels for collected packages: umap-learn, pynndescent\n","  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for umap-learn: filename=umap_learn-0.5.2-py3-none-any.whl size=82708 sha256=4a164edcd1fdeb1499969acd24f91503d89247cf4107233e3194eb20c1c01bfe\n","  Stored in directory: /root/.cache/pip/wheels/84/1b/c6/aaf68a748122632967cef4dffef68224eb16798b6793257d82\n","  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pynndescent: filename=pynndescent-0.5.5-py3-none-any.whl size=52603 sha256=11c613911a6f6673e03ee702fb5d5ac412ce2f02754ebc486f6cc553885af104\n","  Stored in directory: /root/.cache/pip/wheels/af/e9/33/04db1436df0757c42fda8ea6796d7a8586e23c85fac355f476\n","Successfully built umap-learn pynndescent\n","Installing collected packages: pynndescent, umap-learn\n","Successfully installed pynndescent-0.5.5 umap-learn-0.5.2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/numba/np/ufunc/parallel.py:363: NumbaWarning:\n","\n","The TBB threading layer requires TBB version 2019.5 or later i.e., TBB_INTERFACE_VERSION >= 11005. Found TBB_INTERFACE_VERSION = 9107. The TBB threading layer is disabled.\n","\n"]}]},{"cell_type":"code","source":["train_df = data.iloc[:len(train_fea),:].values\n","test_df= data.iloc[len(train_fea):,:].values"],"metadata":{"id":"CXFrShj7QExw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["kfold = StratifiedKFold(n_splits=20)"],"metadata":{"id":"LO-_Ib6yQGGW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_func = nn.BCELoss()"],"metadata":{"id":"PHgbk5GiQHSx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ra = np.arange(3,train_df.shape[1])\n","np.random.shuffle(ra)\n","train_df[:,3:] = train_df[:,ra]\n","test_df[:,3:] = test_df[:,ra]"],"metadata":{"id":"K1yUSoLmQIe2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cat_col = [0,1]"],"metadata":{"id":"TaCcxlIpQJz4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["seed_everything(1006)\n","submission.iloc[:,1:]=0\n","for train,test in kfold.split(train_df,np.zeros(len(train_df))):\n","    batch_size=512\n","    sparse_constant = 0\n","    model = TabNetWithEmbed(train_df.shape[1]-1,train_tar.shape[1],n_d=128,n_a=16,n_shared=1,n_ind=4,n_steps=3,relax=1.5,vbs=64)\n","    model.to(device)\n","    torch.cuda.empty_cache()\n","    optimizer = optim.Adam(model.parameters(),lr=0.007809719000164987,weight_decay=0.00001)\n","    sched = optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=0.1,patience=3,verbose=True)\n","    train_dataset = DrugData(train_df[train],train_tar[train])\n","    valid_dataset = DrugData(train_df[test],train_tar[test],False)\n","    train_loader = DataLoader(train_dataset,batch_size=batch_size,num_workers=4,shuffle=True)\n","    valid_loader = DataLoader(valid_dataset,batch_size=batch_size,num_workers=4,shuffle=True)\n","    losses=[]\n","    norm = []\n","    valid_losses = []\n","    train_losses = []\n","    t = time.time()\n","    for x in range(24):\n","        train_loss=0.\n","        grad_norm_sum = 0.\n","        for inp,tar in train_loader:\n","            model.zero_grad()\n","            out,l = model(inp[:,cat_col].long(),inp[:,3:])\n","            loss = loss_func(out,tar.to(device))#+l*sparse_constant\n","            loss.backward()\n","            optimizer.step()\n","#             sched.step()\n","            train_loss+=loss.item()*tar.size(0)\n","        valid_loss=0.\n","        v=0\n","        for inp,tar in valid_loader:\n","            v+=1\n","            out,_ = model(inp[:,cat_col].long(),inp[:,3:])\n","            loss = loss_func(out,tar.to(device)) \n","            valid_loss += loss.item()*tar.size(0)\n","        losses.append(valid_loss/len(valid_dataset))\n","        valid_losses.append(losses[-1])\n","        train_losses.append(train_loss/len(train_dataset))\n","        print('%d epoch, %.8f valid_loss, %.8f training_loss %fsec time'% (x+1, losses[-1], train_loss/len(train_dataset), (time.time() - t)))\n","        sched.step(losses[-1])\n","        t = time.time()\n","    print(\"completed training one fold -------------- \")\n","    model.eval()\n","    submission.iloc[:,1:] += model(torch.from_numpy(test_df[:,cat_col]).long(),torch.from_numpy(test_df[:,3:]).float())[0].data.cpu().numpy()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dRgZCfxJQLD6","executionInfo":{"status":"ok","timestamp":1642562985171,"user_tz":-480,"elapsed":4946168,"user":{"displayName":"Indrik Wijaya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00208067985282844199"}},"outputId":"ad6099a8-5f9b-4a7d-a4d9-d913452024ac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning:\n","\n","This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","\n"]},{"output_type":"stream","name":"stdout","text":["1 epoch, 0.01925417 valid_loss, 0.05950320 training_loss 8.809434sec time\n","2 epoch, 0.01831277 valid_loss, 0.01888210 training_loss 8.653638sec time\n","3 epoch, 0.01761686 valid_loss, 0.01796183 training_loss 8.874561sec time\n","4 epoch, 0.01711001 valid_loss, 0.01706849 training_loss 8.690574sec time\n","5 epoch, 0.01646078 valid_loss, 0.01642215 training_loss 8.690804sec time\n","6 epoch, 0.01649438 valid_loss, 0.01607803 training_loss 8.756106sec time\n","7 epoch, 0.01595966 valid_loss, 0.01580850 training_loss 8.805627sec time\n","8 epoch, 0.01605164 valid_loss, 0.01554960 training_loss 8.814434sec time\n","9 epoch, 0.01586758 valid_loss, 0.01542189 training_loss 8.763968sec time\n","10 epoch, 0.01566753 valid_loss, 0.01522294 training_loss 8.800066sec time\n","11 epoch, 0.01582895 valid_loss, 0.01516107 training_loss 8.755854sec time\n","12 epoch, 0.01568054 valid_loss, 0.01502120 training_loss 8.703819sec time\n","13 epoch, 0.01559140 valid_loss, 0.01491379 training_loss 8.760062sec time\n","14 epoch, 0.01563122 valid_loss, 0.01486487 training_loss 8.846155sec time\n","15 epoch, 0.01548804 valid_loss, 0.01487074 training_loss 8.675668sec time\n","16 epoch, 0.01573911 valid_loss, 0.01479647 training_loss 8.689693sec time\n","17 epoch, 0.01553838 valid_loss, 0.01477764 training_loss 8.815612sec time\n","18 epoch, 0.01551487 valid_loss, 0.01470323 training_loss 9.083614sec time\n","19 epoch, 0.01554217 valid_loss, 0.01467023 training_loss 9.699967sec time\n","Epoch    19: reducing learning rate of group 0 to 7.8097e-04.\n","20 epoch, 0.01515414 valid_loss, 0.01411898 training_loss 10.112413sec time\n","21 epoch, 0.01512568 valid_loss, 0.01373244 training_loss 10.414866sec time\n","22 epoch, 0.01518146 valid_loss, 0.01356815 training_loss 10.271379sec time\n","23 epoch, 0.01521998 valid_loss, 0.01345105 training_loss 10.473010sec time\n","24 epoch, 0.01527701 valid_loss, 0.01331814 training_loss 10.676762sec time\n","completed training one fold -------------- \n","1 epoch, 0.01956476 valid_loss, 0.06077175 training_loss 10.212835sec time\n","2 epoch, 0.01863455 valid_loss, 0.01891999 training_loss 10.015355sec time\n","3 epoch, 0.01794875 valid_loss, 0.01808386 training_loss 10.197795sec time\n","4 epoch, 0.01719681 valid_loss, 0.01734226 training_loss 10.003661sec time\n","5 epoch, 0.01674059 valid_loss, 0.01664510 training_loss 10.074728sec time\n","6 epoch, 0.01643186 valid_loss, 0.01617802 training_loss 10.094070sec time\n","7 epoch, 0.01636042 valid_loss, 0.01579330 training_loss 10.094764sec time\n","8 epoch, 0.01644437 valid_loss, 0.01564593 training_loss 10.215775sec time\n","9 epoch, 0.01618521 valid_loss, 0.01546687 training_loss 10.188193sec time\n","10 epoch, 0.01599406 valid_loss, 0.01525501 training_loss 10.173527sec time\n","11 epoch, 0.01606741 valid_loss, 0.01512057 training_loss 10.186759sec time\n","12 epoch, 0.01581446 valid_loss, 0.01511646 training_loss 10.214431sec time\n","13 epoch, 0.01571892 valid_loss, 0.01500879 training_loss 10.185150sec time\n","14 epoch, 0.01576845 valid_loss, 0.01491866 training_loss 10.375670sec time\n","15 epoch, 0.01583592 valid_loss, 0.01489740 training_loss 10.278935sec time\n","16 epoch, 0.01573700 valid_loss, 0.01480431 training_loss 10.463679sec time\n","17 epoch, 0.01563090 valid_loss, 0.01483633 training_loss 10.504230sec time\n","18 epoch, 0.01575238 valid_loss, 0.01486258 training_loss 11.486547sec time\n","19 epoch, 0.01575130 valid_loss, 0.01473297 training_loss 12.661261sec time\n","20 epoch, 0.01570913 valid_loss, 0.01473410 training_loss 10.534369sec time\n","21 epoch, 0.01575753 valid_loss, 0.01475705 training_loss 11.647087sec time\n","Epoch    21: reducing learning rate of group 0 to 7.8097e-04.\n","22 epoch, 0.01522518 valid_loss, 0.01413269 training_loss 13.592838sec time\n","23 epoch, 0.01527616 valid_loss, 0.01377595 training_loss 14.545326sec time\n","24 epoch, 0.01526270 valid_loss, 0.01360281 training_loss 15.613208sec time\n","completed training one fold -------------- \n","1 epoch, 0.01935361 valid_loss, 0.06061492 training_loss 8.947401sec time\n","2 epoch, 0.01841961 valid_loss, 0.01888126 training_loss 8.829326sec time\n","3 epoch, 0.01789976 valid_loss, 0.01809774 training_loss 8.917931sec time\n","4 epoch, 0.01705136 valid_loss, 0.01726503 training_loss 8.891948sec time\n","5 epoch, 0.01646120 valid_loss, 0.01655591 training_loss 8.945997sec time\n","6 epoch, 0.01650436 valid_loss, 0.01616799 training_loss 8.932183sec time\n","7 epoch, 0.01610369 valid_loss, 0.01589920 training_loss 8.910996sec time\n","8 epoch, 0.01586027 valid_loss, 0.01566097 training_loss 8.923306sec time\n","9 epoch, 0.01574711 valid_loss, 0.01551492 training_loss 8.868275sec time\n","10 epoch, 0.01557582 valid_loss, 0.01538958 training_loss 8.877051sec time\n","11 epoch, 0.01555026 valid_loss, 0.01519055 training_loss 8.948105sec time\n","12 epoch, 0.01536565 valid_loss, 0.01515097 training_loss 9.029219sec time\n","13 epoch, 0.01555808 valid_loss, 0.01506319 training_loss 8.945694sec time\n","14 epoch, 0.01547244 valid_loss, 0.01491248 training_loss 9.002786sec time\n","15 epoch, 0.01557215 valid_loss, 0.01497200 training_loss 9.088996sec time\n","16 epoch, 0.01569824 valid_loss, 0.01486931 training_loss 9.139330sec time\n","Epoch    16: reducing learning rate of group 0 to 7.8097e-04.\n","17 epoch, 0.01490403 valid_loss, 0.01434174 training_loss 9.351615sec time\n","18 epoch, 0.01491938 valid_loss, 0.01398791 training_loss 9.502990sec time\n","19 epoch, 0.01490306 valid_loss, 0.01383111 training_loss 9.433684sec time\n","20 epoch, 0.01496978 valid_loss, 0.01370800 training_loss 9.655594sec time\n","21 epoch, 0.01495723 valid_loss, 0.01362049 training_loss 9.680194sec time\n","Epoch    21: reducing learning rate of group 0 to 7.8097e-05.\n","22 epoch, 0.01490730 valid_loss, 0.01336804 training_loss 9.655996sec time\n","23 epoch, 0.01492124 valid_loss, 0.01331962 training_loss 9.647480sec time\n","24 epoch, 0.01489623 valid_loss, 0.01328717 training_loss 9.613600sec time\n","completed training one fold -------------- \n","1 epoch, 0.01916300 valid_loss, 0.05905937 training_loss 8.958017sec time\n","2 epoch, 0.01811979 valid_loss, 0.01887455 training_loss 8.950547sec time\n","3 epoch, 0.01734183 valid_loss, 0.01802606 training_loss 8.890803sec time\n","4 epoch, 0.01684703 valid_loss, 0.01713619 training_loss 8.883417sec time\n","5 epoch, 0.01626043 valid_loss, 0.01662341 training_loss 8.961504sec time\n","6 epoch, 0.01609991 valid_loss, 0.01614646 training_loss 8.929425sec time\n","7 epoch, 0.01565549 valid_loss, 0.01588441 training_loss 8.972455sec time\n","8 epoch, 0.01631580 valid_loss, 0.01568322 training_loss 9.052898sec time\n","9 epoch, 0.01549594 valid_loss, 0.01558087 training_loss 8.964756sec time\n","10 epoch, 0.01529293 valid_loss, 0.01530098 training_loss 8.851587sec time\n","11 epoch, 0.01524351 valid_loss, 0.01523137 training_loss 8.924190sec time\n","12 epoch, 0.01508413 valid_loss, 0.01512228 training_loss 9.115398sec time\n","13 epoch, 0.01509905 valid_loss, 0.01508537 training_loss 9.005329sec time\n","14 epoch, 0.01510316 valid_loss, 0.01491527 training_loss 8.946887sec time\n","15 epoch, 0.01514596 valid_loss, 0.01487850 training_loss 9.213090sec time\n","16 epoch, 0.01522007 valid_loss, 0.01485145 training_loss 9.237748sec time\n","Epoch    16: reducing learning rate of group 0 to 7.8097e-04.\n","17 epoch, 0.01460934 valid_loss, 0.01429835 training_loss 9.410715sec time\n","18 epoch, 0.01459511 valid_loss, 0.01391558 training_loss 9.403037sec time\n","19 epoch, 0.01459908 valid_loss, 0.01375175 training_loss 9.486576sec time\n","20 epoch, 0.01466452 valid_loss, 0.01362369 training_loss 9.490052sec time\n","21 epoch, 0.01469726 valid_loss, 0.01348522 training_loss 9.630901sec time\n","22 epoch, 0.01461301 valid_loss, 0.01338808 training_loss 9.539548sec time\n","Epoch    22: reducing learning rate of group 0 to 7.8097e-05.\n","23 epoch, 0.01466626 valid_loss, 0.01311507 training_loss 9.516348sec time\n","24 epoch, 0.01469306 valid_loss, 0.01304835 training_loss 9.517708sec time\n","completed training one fold -------------- \n","1 epoch, 0.01918966 valid_loss, 0.05981084 training_loss 8.976600sec time\n","2 epoch, 0.01822821 valid_loss, 0.01885855 training_loss 9.078836sec time\n","3 epoch, 0.01742974 valid_loss, 0.01798206 training_loss 8.961487sec time\n","4 epoch, 0.01668345 valid_loss, 0.01705562 training_loss 9.049721sec time\n","5 epoch, 0.01648266 valid_loss, 0.01648328 training_loss 8.900245sec time\n","6 epoch, 0.01598531 valid_loss, 0.01611458 training_loss 8.988034sec time\n","7 epoch, 0.01589608 valid_loss, 0.01578619 training_loss 8.945968sec time\n","8 epoch, 0.01606180 valid_loss, 0.01552808 training_loss 8.976053sec time\n","9 epoch, 0.01567017 valid_loss, 0.01544032 training_loss 8.968130sec time\n","10 epoch, 0.01554932 valid_loss, 0.01521071 training_loss 9.000980sec time\n","11 epoch, 0.01583228 valid_loss, 0.01516602 training_loss 8.997962sec time\n","12 epoch, 0.01549604 valid_loss, 0.01503831 training_loss 8.995715sec time\n","13 epoch, 0.01556092 valid_loss, 0.01492910 training_loss 8.970730sec time\n","14 epoch, 0.01548671 valid_loss, 0.01492654 training_loss 9.053206sec time\n","15 epoch, 0.01565368 valid_loss, 0.01480700 training_loss 8.994807sec time\n","16 epoch, 0.01547600 valid_loss, 0.01480726 training_loss 9.089012sec time\n","17 epoch, 0.01542142 valid_loss, 0.01480817 training_loss 9.484308sec time\n","18 epoch, 0.01542816 valid_loss, 0.01474561 training_loss 9.710781sec time\n","19 epoch, 0.01552748 valid_loss, 0.01471392 training_loss 9.802109sec time\n","20 epoch, 0.01545593 valid_loss, 0.01464059 training_loss 10.401393sec time\n","21 epoch, 0.01550787 valid_loss, 0.01467165 training_loss 12.266553sec time\n","Epoch    21: reducing learning rate of group 0 to 7.8097e-04.\n","22 epoch, 0.01511986 valid_loss, 0.01405337 training_loss 13.838344sec time\n","23 epoch, 0.01505861 valid_loss, 0.01368064 training_loss 14.689805sec time\n","24 epoch, 0.01516612 valid_loss, 0.01351534 training_loss 14.837312sec time\n","completed training one fold -------------- \n","1 epoch, 0.01927867 valid_loss, 0.06031590 training_loss 9.049456sec time\n","2 epoch, 0.01823419 valid_loss, 0.01899516 training_loss 8.932690sec time\n","3 epoch, 0.01747112 valid_loss, 0.01819670 training_loss 9.033216sec time\n","4 epoch, 0.01672672 valid_loss, 0.01735891 training_loss 9.056973sec time\n","5 epoch, 0.01591097 valid_loss, 0.01662524 training_loss 9.004861sec time\n","6 epoch, 0.01576182 valid_loss, 0.01617841 training_loss 8.927287sec time\n","7 epoch, 0.01564897 valid_loss, 0.01590995 training_loss 8.967601sec time\n","8 epoch, 0.01536933 valid_loss, 0.01568199 training_loss 9.014905sec time\n","9 epoch, 0.01548603 valid_loss, 0.01547388 training_loss 8.985901sec time\n","10 epoch, 0.01536843 valid_loss, 0.01530431 training_loss 8.985951sec time\n","11 epoch, 0.01567725 valid_loss, 0.01515895 training_loss 9.063669sec time\n","12 epoch, 0.01544359 valid_loss, 0.01521586 training_loss 8.987515sec time\n","Epoch    12: reducing learning rate of group 0 to 7.8097e-04.\n","13 epoch, 0.01493718 valid_loss, 0.01461578 training_loss 9.035930sec time\n","14 epoch, 0.01481223 valid_loss, 0.01428502 training_loss 9.022417sec time\n","15 epoch, 0.01480449 valid_loss, 0.01412306 training_loss 9.109595sec time\n","16 epoch, 0.01481583 valid_loss, 0.01398938 training_loss 9.130656sec time\n","17 epoch, 0.01478483 valid_loss, 0.01389347 training_loss 9.022902sec time\n","18 epoch, 0.01477526 valid_loss, 0.01376847 training_loss 9.149311sec time\n","19 epoch, 0.01485779 valid_loss, 0.01366121 training_loss 9.113445sec time\n","20 epoch, 0.01480699 valid_loss, 0.01357806 training_loss 9.089583sec time\n","21 epoch, 0.01484932 valid_loss, 0.01347035 training_loss 9.065898sec time\n","22 epoch, 0.01496328 valid_loss, 0.01335511 training_loss 9.015366sec time\n","Epoch    22: reducing learning rate of group 0 to 7.8097e-05.\n","23 epoch, 0.01481161 valid_loss, 0.01300681 training_loss 9.088158sec time\n","24 epoch, 0.01484856 valid_loss, 0.01288446 training_loss 9.131402sec time\n","completed training one fold -------------- \n","1 epoch, 0.01904480 valid_loss, 0.05928590 training_loss 8.915799sec time\n","2 epoch, 0.01812538 valid_loss, 0.01880539 training_loss 8.901211sec time\n","3 epoch, 0.01735896 valid_loss, 0.01794604 training_loss 9.000699sec time\n","4 epoch, 0.01674258 valid_loss, 0.01708367 training_loss 8.949868sec time\n","5 epoch, 0.01624046 valid_loss, 0.01646113 training_loss 8.962722sec time\n","6 epoch, 0.01628377 valid_loss, 0.01605122 training_loss 8.971232sec time\n","7 epoch, 0.01585251 valid_loss, 0.01584754 training_loss 8.987086sec time\n","8 epoch, 0.01593076 valid_loss, 0.01559514 training_loss 8.889791sec time\n","9 epoch, 0.01574081 valid_loss, 0.01540795 training_loss 8.941061sec time\n","10 epoch, 0.01558268 valid_loss, 0.01523980 training_loss 9.028904sec time\n","11 epoch, 0.01566220 valid_loss, 0.01515758 training_loss 8.936249sec time\n","12 epoch, 0.01558698 valid_loss, 0.01504325 training_loss 9.036485sec time\n","13 epoch, 0.01558107 valid_loss, 0.01499518 training_loss 9.089025sec time\n","14 epoch, 0.01545173 valid_loss, 0.01498702 training_loss 9.039206sec time\n","15 epoch, 0.01535227 valid_loss, 0.01487593 training_loss 9.071402sec time\n","16 epoch, 0.01555811 valid_loss, 0.01485998 training_loss 9.206472sec time\n","17 epoch, 0.01554243 valid_loss, 0.01487303 training_loss 9.566638sec time\n","18 epoch, 0.01527382 valid_loss, 0.01476351 training_loss 9.912991sec time\n","19 epoch, 0.01544466 valid_loss, 0.01478138 training_loss 10.486766sec time\n","20 epoch, 0.01551272 valid_loss, 0.01474258 training_loss 11.536981sec time\n","21 epoch, 0.01569188 valid_loss, 0.01467423 training_loss 13.864499sec time\n","22 epoch, 0.01539128 valid_loss, 0.01479215 training_loss 17.832371sec time\n","Epoch    22: reducing learning rate of group 0 to 7.8097e-04.\n","23 epoch, 0.01514653 valid_loss, 0.01407136 training_loss 20.465396sec time\n","24 epoch, 0.01512286 valid_loss, 0.01373995 training_loss 21.143831sec time\n","completed training one fold -------------- \n","1 epoch, 0.02015130 valid_loss, 0.05983449 training_loss 8.991560sec time\n","2 epoch, 0.01928332 valid_loss, 0.01884278 training_loss 9.036154sec time\n","3 epoch, 0.01859109 valid_loss, 0.01806390 training_loss 9.049857sec time\n","4 epoch, 0.01779558 valid_loss, 0.01717870 training_loss 9.037188sec time\n","5 epoch, 0.01738095 valid_loss, 0.01650833 training_loss 9.101726sec time\n","6 epoch, 0.01731014 valid_loss, 0.01613758 training_loss 8.988855sec time\n","7 epoch, 0.01701348 valid_loss, 0.01585173 training_loss 8.994495sec time\n","8 epoch, 0.01700280 valid_loss, 0.01560426 training_loss 9.001899sec time\n","9 epoch, 0.01674094 valid_loss, 0.01540818 training_loss 8.966347sec time\n","10 epoch, 0.01682303 valid_loss, 0.01530418 training_loss 9.074092sec time\n","11 epoch, 0.01659744 valid_loss, 0.01517579 training_loss 9.052428sec time\n","12 epoch, 0.01646689 valid_loss, 0.01503607 training_loss 9.060413sec time\n","13 epoch, 0.01658810 valid_loss, 0.01495468 training_loss 9.159468sec time\n","14 epoch, 0.01651016 valid_loss, 0.01488227 training_loss 9.060241sec time\n","15 epoch, 0.01633301 valid_loss, 0.01492697 training_loss 9.136324sec time\n","16 epoch, 0.01648096 valid_loss, 0.01483997 training_loss 9.267107sec time\n","17 epoch, 0.01656787 valid_loss, 0.01479722 training_loss 9.671841sec time\n","18 epoch, 0.01661120 valid_loss, 0.01474605 training_loss 9.904961sec time\n","19 epoch, 0.01655938 valid_loss, 0.01473553 training_loss 10.003014sec time\n","Epoch    19: reducing learning rate of group 0 to 7.8097e-04.\n","20 epoch, 0.01601677 valid_loss, 0.01412350 training_loss 10.274822sec time\n","21 epoch, 0.01605234 valid_loss, 0.01378743 training_loss 10.635228sec time\n","22 epoch, 0.01603350 valid_loss, 0.01363565 training_loss 10.672509sec time\n","23 epoch, 0.01608032 valid_loss, 0.01351479 training_loss 10.909641sec time\n","24 epoch, 0.01606931 valid_loss, 0.01337982 training_loss 11.160460sec time\n","Epoch    24: reducing learning rate of group 0 to 7.8097e-05.\n","completed training one fold -------------- \n","1 epoch, 0.01866994 valid_loss, 0.06021641 training_loss 9.117533sec time\n","2 epoch, 0.01773141 valid_loss, 0.01893580 training_loss 9.186935sec time\n","3 epoch, 0.01704452 valid_loss, 0.01817157 training_loss 9.229593sec time\n","4 epoch, 0.01605305 valid_loss, 0.01738891 training_loss 9.205152sec time\n","5 epoch, 0.01562160 valid_loss, 0.01671877 training_loss 9.192070sec time\n","6 epoch, 0.01551407 valid_loss, 0.01631784 training_loss 9.229472sec time\n","7 epoch, 0.01537250 valid_loss, 0.01599924 training_loss 9.245478sec time\n","8 epoch, 0.01515868 valid_loss, 0.01573999 training_loss 9.254347sec time\n","9 epoch, 0.01494265 valid_loss, 0.01558465 training_loss 9.188605sec time\n","10 epoch, 0.01498938 valid_loss, 0.01535287 training_loss 9.252465sec time\n","11 epoch, 0.01478085 valid_loss, 0.01523160 training_loss 9.272650sec time\n","12 epoch, 0.01473746 valid_loss, 0.01517080 training_loss 9.429053sec time\n","13 epoch, 0.01466288 valid_loss, 0.01504227 training_loss 9.314015sec time\n","14 epoch, 0.01466513 valid_loss, 0.01509676 training_loss 9.417652sec time\n","15 epoch, 0.01487646 valid_loss, 0.01495083 training_loss 9.333786sec time\n","16 epoch, 0.01463183 valid_loss, 0.01490311 training_loss 9.519345sec time\n","17 epoch, 0.01474705 valid_loss, 0.01486271 training_loss 9.611048sec time\n","18 epoch, 0.01451539 valid_loss, 0.01484985 training_loss 10.303163sec time\n","19 epoch, 0.01469774 valid_loss, 0.01477660 training_loss 10.537369sec time\n","20 epoch, 0.01452984 valid_loss, 0.01485597 training_loss 11.159204sec time\n","21 epoch, 0.01440228 valid_loss, 0.01473264 training_loss 12.613582sec time\n","22 epoch, 0.01449801 valid_loss, 0.01470374 training_loss 15.737889sec time\n","23 epoch, 0.01490803 valid_loss, 0.01468320 training_loss 19.505563sec time\n","24 epoch, 0.01450798 valid_loss, 0.01468996 training_loss 21.754574sec time\n","completed training one fold -------------- \n","1 epoch, 0.02019724 valid_loss, 0.06041219 training_loss 9.178061sec time\n","2 epoch, 0.01915649 valid_loss, 0.01907162 training_loss 9.271235sec time\n","3 epoch, 0.01844486 valid_loss, 0.01816193 training_loss 9.157163sec time\n","4 epoch, 0.01765528 valid_loss, 0.01722483 training_loss 9.209551sec time\n","5 epoch, 0.01708083 valid_loss, 0.01658826 training_loss 9.121183sec time\n","6 epoch, 0.01677859 valid_loss, 0.01613057 training_loss 9.123235sec time\n","7 epoch, 0.01669707 valid_loss, 0.01584475 training_loss 9.127829sec time\n","8 epoch, 0.01658129 valid_loss, 0.01560155 training_loss 9.108957sec time\n","9 epoch, 0.01649428 valid_loss, 0.01532581 training_loss 9.073545sec time\n","10 epoch, 0.01631967 valid_loss, 0.01526293 training_loss 9.080088sec time\n","11 epoch, 0.01606263 valid_loss, 0.01511470 training_loss 8.985507sec time\n","12 epoch, 0.01615921 valid_loss, 0.01498845 training_loss 9.046994sec time\n","13 epoch, 0.01614614 valid_loss, 0.01497275 training_loss 9.074122sec time\n","14 epoch, 0.01602933 valid_loss, 0.01490328 training_loss 9.073511sec time\n","15 epoch, 0.01615333 valid_loss, 0.01489524 training_loss 9.179290sec time\n","16 epoch, 0.01613417 valid_loss, 0.01479889 training_loss 9.334126sec time\n","17 epoch, 0.01601692 valid_loss, 0.01478215 training_loss 9.613067sec time\n","18 epoch, 0.01604021 valid_loss, 0.01474552 training_loss 10.109184sec time\n","19 epoch, 0.01622401 valid_loss, 0.01466602 training_loss 10.573925sec time\n","20 epoch, 0.01595446 valid_loss, 0.01466235 training_loss 11.096189sec time\n","21 epoch, 0.01604330 valid_loss, 0.01468027 training_loss 12.998727sec time\n","22 epoch, 0.01619123 valid_loss, 0.01460534 training_loss 15.274379sec time\n","23 epoch, 0.01596057 valid_loss, 0.01463179 training_loss 18.653435sec time\n","24 epoch, 0.01592576 valid_loss, 0.01455344 training_loss 21.700387sec time\n","completed training one fold -------------- \n","1 epoch, 0.01912986 valid_loss, 0.06034324 training_loss 9.201179sec time\n","2 epoch, 0.01818545 valid_loss, 0.01886389 training_loss 9.161503sec time\n","3 epoch, 0.01760328 valid_loss, 0.01813788 training_loss 9.007445sec time\n","4 epoch, 0.01664949 valid_loss, 0.01731504 training_loss 9.005417sec time\n","5 epoch, 0.01631184 valid_loss, 0.01659399 training_loss 9.105259sec time\n","6 epoch, 0.01601161 valid_loss, 0.01616671 training_loss 9.055212sec time\n","7 epoch, 0.01572763 valid_loss, 0.01596250 training_loss 9.083901sec time\n","8 epoch, 0.01571899 valid_loss, 0.01561570 training_loss 9.199336sec time\n","9 epoch, 0.01555940 valid_loss, 0.01538307 training_loss 9.112670sec time\n","10 epoch, 0.01539853 valid_loss, 0.01526171 training_loss 9.038760sec time\n","11 epoch, 0.01538394 valid_loss, 0.01509819 training_loss 9.125128sec time\n","12 epoch, 0.01532817 valid_loss, 0.01515632 training_loss 9.079009sec time\n","13 epoch, 0.01511556 valid_loss, 0.01501518 training_loss 9.136386sec time\n","14 epoch, 0.01538464 valid_loss, 0.01489116 training_loss 9.095845sec time\n","15 epoch, 0.01513017 valid_loss, 0.01488420 training_loss 9.025298sec time\n","16 epoch, 0.01523884 valid_loss, 0.01479595 training_loss 9.187807sec time\n","17 epoch, 0.01514583 valid_loss, 0.01480425 training_loss 10.058442sec time\n","Epoch    17: reducing learning rate of group 0 to 7.8097e-04.\n","18 epoch, 0.01485953 valid_loss, 0.01422781 training_loss 10.549189sec time\n","19 epoch, 0.01475788 valid_loss, 0.01385568 training_loss 10.804996sec time\n","20 epoch, 0.01482930 valid_loss, 0.01368990 training_loss 10.965419sec time\n","21 epoch, 0.01478666 valid_loss, 0.01355696 training_loss 11.332073sec time\n","22 epoch, 0.01479837 valid_loss, 0.01343918 training_loss 11.582750sec time\n","23 epoch, 0.01491469 valid_loss, 0.01332567 training_loss 11.833795sec time\n","Epoch    23: reducing learning rate of group 0 to 7.8097e-05.\n","24 epoch, 0.01487284 valid_loss, 0.01305020 training_loss 11.971172sec time\n","completed training one fold -------------- \n","1 epoch, 0.01947717 valid_loss, 0.06014047 training_loss 9.161946sec time\n","2 epoch, 0.01850883 valid_loss, 0.01887342 training_loss 9.133444sec time\n","3 epoch, 0.01803084 valid_loss, 0.01816341 training_loss 9.121215sec time\n","4 epoch, 0.01722368 valid_loss, 0.01729414 training_loss 9.094200sec time\n","5 epoch, 0.01684581 valid_loss, 0.01656872 training_loss 9.089638sec time\n","6 epoch, 0.01670470 valid_loss, 0.01616484 training_loss 9.113879sec time\n","7 epoch, 0.01654435 valid_loss, 0.01585688 training_loss 9.144141sec time\n","8 epoch, 0.01623747 valid_loss, 0.01567706 training_loss 9.244879sec time\n","9 epoch, 0.01613877 valid_loss, 0.01546881 training_loss 9.132628sec time\n","10 epoch, 0.01610152 valid_loss, 0.01529099 training_loss 9.095360sec time\n","11 epoch, 0.01590882 valid_loss, 0.01516437 training_loss 9.033483sec time\n","12 epoch, 0.01578530 valid_loss, 0.01507103 training_loss 9.134375sec time\n","13 epoch, 0.01612512 valid_loss, 0.01495644 training_loss 9.091202sec time\n","14 epoch, 0.01574273 valid_loss, 0.01500066 training_loss 9.086459sec time\n","15 epoch, 0.01575202 valid_loss, 0.01493028 training_loss 9.092299sec time\n","16 epoch, 0.01591216 valid_loss, 0.01484005 training_loss 9.120207sec time\n","17 epoch, 0.01563469 valid_loss, 0.01482512 training_loss 10.198091sec time\n","18 epoch, 0.01578057 valid_loss, 0.01478960 training_loss 10.890230sec time\n","19 epoch, 0.01568274 valid_loss, 0.01488308 training_loss 11.486398sec time\n","20 epoch, 0.01584838 valid_loss, 0.01477449 training_loss 13.791303sec time\n","21 epoch, 0.01557800 valid_loss, 0.01481796 training_loss 17.296848sec time\n","22 epoch, 0.01567854 valid_loss, 0.01470784 training_loss 19.707249sec time\n","23 epoch, 0.01564150 valid_loss, 0.01466454 training_loss 21.374487sec time\n","24 epoch, 0.01566954 valid_loss, 0.01470889 training_loss 24.052952sec time\n","completed training one fold -------------- \n","1 epoch, 0.01980862 valid_loss, 0.05942226 training_loss 9.082454sec time\n","2 epoch, 0.01886270 valid_loss, 0.01883741 training_loss 9.040694sec time\n","3 epoch, 0.01803800 valid_loss, 0.01801394 training_loss 9.119418sec time\n","4 epoch, 0.01719761 valid_loss, 0.01708958 training_loss 9.034725sec time\n","5 epoch, 0.01675031 valid_loss, 0.01647085 training_loss 8.948182sec time\n","6 epoch, 0.01672718 valid_loss, 0.01609071 training_loss 9.055896sec time\n","7 epoch, 0.01633231 valid_loss, 0.01586666 training_loss 9.102626sec time\n","8 epoch, 0.01618830 valid_loss, 0.01568944 training_loss 9.176087sec time\n","9 epoch, 0.01596615 valid_loss, 0.01545780 training_loss 9.117268sec time\n","10 epoch, 0.01582138 valid_loss, 0.01531594 training_loss 9.178042sec time\n","11 epoch, 0.01614107 valid_loss, 0.01520062 training_loss 9.262225sec time\n","12 epoch, 0.01579365 valid_loss, 0.01513473 training_loss 9.227571sec time\n","13 epoch, 0.01583157 valid_loss, 0.01500884 training_loss 9.129681sec time\n","14 epoch, 0.01574047 valid_loss, 0.01492926 training_loss 9.245407sec time\n","15 epoch, 0.01575924 valid_loss, 0.01492868 training_loss 9.286650sec time\n","16 epoch, 0.01571124 valid_loss, 0.01486826 training_loss 9.459455sec time\n","17 epoch, 0.01600821 valid_loss, 0.01490417 training_loss 11.139004sec time\n","18 epoch, 0.01561479 valid_loss, 0.01492611 training_loss 13.092021sec time\n","19 epoch, 0.01554159 valid_loss, 0.01475897 training_loss 13.930439sec time\n","20 epoch, 0.01577664 valid_loss, 0.01467846 training_loss 14.827424sec time\n","21 epoch, 0.01560481 valid_loss, 0.01474303 training_loss 16.386378sec time\n","22 epoch, 0.01562402 valid_loss, 0.01469460 training_loss 19.223238sec time\n","23 epoch, 0.01560729 valid_loss, 0.01459083 training_loss 21.086220sec time\n","Epoch    23: reducing learning rate of group 0 to 7.8097e-04.\n","24 epoch, 0.01505079 valid_loss, 0.01402158 training_loss 21.978646sec time\n","completed training one fold -------------- \n","1 epoch, 0.01944193 valid_loss, 0.06106265 training_loss 9.281401sec time\n","2 epoch, 0.01856760 valid_loss, 0.01897889 training_loss 9.273755sec time\n","3 epoch, 0.01819035 valid_loss, 0.01821694 training_loss 9.219003sec time\n","4 epoch, 0.01730430 valid_loss, 0.01747205 training_loss 9.210500sec time\n","5 epoch, 0.01685935 valid_loss, 0.01667600 training_loss 9.266459sec time\n","6 epoch, 0.01670572 valid_loss, 0.01619433 training_loss 9.181699sec time\n","7 epoch, 0.01635100 valid_loss, 0.01590890 training_loss 9.092218sec time\n","8 epoch, 0.01606813 valid_loss, 0.01569640 training_loss 9.090442sec time\n","9 epoch, 0.01609234 valid_loss, 0.01547286 training_loss 9.181458sec time\n","10 epoch, 0.01601802 valid_loss, 0.01534746 training_loss 9.157915sec time\n","11 epoch, 0.01577519 valid_loss, 0.01516600 training_loss 9.228534sec time\n","12 epoch, 0.01736499 valid_loss, 0.01510196 training_loss 9.176998sec time\n","13 epoch, 0.01572297 valid_loss, 0.01545100 training_loss 9.265366sec time\n","14 epoch, 0.01581014 valid_loss, 0.01502272 training_loss 9.165684sec time\n","15 epoch, 0.01558664 valid_loss, 0.01493136 training_loss 9.253240sec time\n","16 epoch, 0.01553192 valid_loss, 0.01495689 training_loss 9.379632sec time\n","17 epoch, 0.01566457 valid_loss, 0.01497564 training_loss 9.348906sec time\n","18 epoch, 0.01573215 valid_loss, 0.01488214 training_loss 9.504838sec time\n","19 epoch, 0.01551363 valid_loss, 0.01483531 training_loss 9.891838sec time\n","20 epoch, 0.01561425 valid_loss, 0.01485375 training_loss 10.343663sec time\n","21 epoch, 0.01557119 valid_loss, 0.01477498 training_loss 11.422529sec time\n","22 epoch, 0.01593293 valid_loss, 0.01476836 training_loss 13.023816sec time\n","23 epoch, 0.01553964 valid_loss, 0.01482845 training_loss 15.959593sec time\n","Epoch    23: reducing learning rate of group 0 to 7.8097e-04.\n","24 epoch, 0.01516708 valid_loss, 0.01413433 training_loss 17.831724sec time\n","completed training one fold -------------- \n","1 epoch, 0.01985976 valid_loss, 0.05825864 training_loss 9.136699sec time\n","2 epoch, 0.01876381 valid_loss, 0.01886015 training_loss 9.177710sec time\n","3 epoch, 0.01808446 valid_loss, 0.01800895 training_loss 9.208590sec time\n","4 epoch, 0.01742349 valid_loss, 0.01719884 training_loss 9.125463sec time\n","5 epoch, 0.01688159 valid_loss, 0.01648906 training_loss 9.087503sec time\n","6 epoch, 0.01674387 valid_loss, 0.01610038 training_loss 9.100150sec time\n","7 epoch, 0.01649570 valid_loss, 0.01576518 training_loss 9.288766sec time\n","8 epoch, 0.01632958 valid_loss, 0.01559175 training_loss 9.154250sec time\n","9 epoch, 0.01625367 valid_loss, 0.01535630 training_loss 9.119830sec time\n","10 epoch, 0.01638995 valid_loss, 0.01523479 training_loss 9.124326sec time\n","11 epoch, 0.01606708 valid_loss, 0.01510959 training_loss 9.189818sec time\n","12 epoch, 0.01617076 valid_loss, 0.01505110 training_loss 9.093122sec time\n","13 epoch, 0.01604179 valid_loss, 0.01499418 training_loss 9.145662sec time\n","14 epoch, 0.01615603 valid_loss, 0.01492708 training_loss 9.197359sec time\n","15 epoch, 0.01610552 valid_loss, 0.01492819 training_loss 9.412059sec time\n","16 epoch, 0.01594288 valid_loss, 0.01488876 training_loss 9.390543sec time\n","17 epoch, 0.01607794 valid_loss, 0.01479873 training_loss 9.566721sec time\n","18 epoch, 0.01605670 valid_loss, 0.01480130 training_loss 9.761365sec time\n","19 epoch, 0.01612600 valid_loss, 0.01477598 training_loss 10.310469sec time\n","20 epoch, 0.01600592 valid_loss, 0.01471258 training_loss 13.926149sec time\n","Epoch    20: reducing learning rate of group 0 to 7.8097e-04.\n","21 epoch, 0.01563786 valid_loss, 0.01411380 training_loss 15.634099sec time\n","22 epoch, 0.01559813 valid_loss, 0.01372871 training_loss 16.356877sec time\n","23 epoch, 0.01562563 valid_loss, 0.01355938 training_loss 16.858408sec time\n","24 epoch, 0.01569882 valid_loss, 0.01341186 training_loss 16.905726sec time\n","completed training one fold -------------- \n","1 epoch, 0.01863530 valid_loss, 0.06005575 training_loss 9.329889sec time\n","2 epoch, 0.01756536 valid_loss, 0.01891126 training_loss 9.183856sec time\n","3 epoch, 0.01677272 valid_loss, 0.01799573 training_loss 9.188549sec time\n","4 epoch, 0.01586836 valid_loss, 0.01716129 training_loss 9.208149sec time\n","5 epoch, 0.01565286 valid_loss, 0.01653031 training_loss 9.199879sec time\n","6 epoch, 0.01550298 valid_loss, 0.01617148 training_loss 9.116892sec time\n","7 epoch, 0.01534107 valid_loss, 0.01594428 training_loss 9.203211sec time\n","8 epoch, 0.01517173 valid_loss, 0.01569997 training_loss 9.168772sec time\n","9 epoch, 0.01508013 valid_loss, 0.01547232 training_loss 9.237123sec time\n","10 epoch, 0.01496094 valid_loss, 0.01539461 training_loss 9.288159sec time\n","11 epoch, 0.01503707 valid_loss, 0.01518188 training_loss 9.235885sec time\n","12 epoch, 0.01496059 valid_loss, 0.01519691 training_loss 9.174331sec time\n","13 epoch, 0.01496050 valid_loss, 0.01508165 training_loss 9.213619sec time\n","14 epoch, 0.01487299 valid_loss, 0.01500405 training_loss 9.239442sec time\n","15 epoch, 0.01477384 valid_loss, 0.01494301 training_loss 9.322805sec time\n","16 epoch, 0.01484846 valid_loss, 0.01488312 training_loss 9.438589sec time\n","17 epoch, 0.01492498 valid_loss, 0.01488777 training_loss 9.492368sec time\n","18 epoch, 0.01481989 valid_loss, 0.01497231 training_loss 9.453498sec time\n","19 epoch, 0.01485637 valid_loss, 0.01488718 training_loss 9.536266sec time\n","Epoch    19: reducing learning rate of group 0 to 7.8097e-04.\n","20 epoch, 0.01448836 valid_loss, 0.01429204 training_loss 9.671679sec time\n","21 epoch, 0.01440585 valid_loss, 0.01393111 training_loss 9.987915sec time\n","22 epoch, 0.01449448 valid_loss, 0.01377072 training_loss 10.171989sec time\n","23 epoch, 0.01442581 valid_loss, 0.01363116 training_loss 10.497275sec time\n","24 epoch, 0.01460338 valid_loss, 0.01351084 training_loss 10.785884sec time\n","completed training one fold -------------- \n","1 epoch, 0.02014703 valid_loss, 0.06166741 training_loss 9.186263sec time\n","2 epoch, 0.01920878 valid_loss, 0.01896820 training_loss 9.181518sec time\n","3 epoch, 0.01848543 valid_loss, 0.01819112 training_loss 9.143358sec time\n","4 epoch, 0.01757829 valid_loss, 0.01730625 training_loss 9.190877sec time\n","5 epoch, 0.01711232 valid_loss, 0.01650376 training_loss 9.088879sec time\n","6 epoch, 0.01691007 valid_loss, 0.01613709 training_loss 9.135511sec time\n","7 epoch, 0.01651768 valid_loss, 0.01582123 training_loss 9.142454sec time\n","8 epoch, 0.01656685 valid_loss, 0.01552835 training_loss 9.208812sec time\n","9 epoch, 0.01640865 valid_loss, 0.01535335 training_loss 9.187737sec time\n","10 epoch, 0.01627405 valid_loss, 0.01516249 training_loss 9.182381sec time\n","11 epoch, 0.01621056 valid_loss, 0.01504893 training_loss 9.129172sec time\n","12 epoch, 0.01612160 valid_loss, 0.01499636 training_loss 9.099451sec time\n","13 epoch, 0.01613385 valid_loss, 0.01492737 training_loss 9.278492sec time\n","14 epoch, 0.01617394 valid_loss, 0.01484408 training_loss 9.176630sec time\n","15 epoch, 0.01616489 valid_loss, 0.01477044 training_loss 9.235195sec time\n","16 epoch, 0.01629326 valid_loss, 0.01477687 training_loss 9.378420sec time\n","Epoch    16: reducing learning rate of group 0 to 7.8097e-04.\n","17 epoch, 0.01569951 valid_loss, 0.01423982 training_loss 9.435831sec time\n","18 epoch, 0.01565568 valid_loss, 0.01386174 training_loss 9.446904sec time\n","19 epoch, 0.01568474 valid_loss, 0.01370681 training_loss 9.775637sec time\n","20 epoch, 0.01561356 valid_loss, 0.01357482 training_loss 9.623873sec time\n","21 epoch, 0.01571453 valid_loss, 0.01346391 training_loss 9.662236sec time\n","22 epoch, 0.01574661 valid_loss, 0.01337159 training_loss 9.713511sec time\n","23 epoch, 0.01574599 valid_loss, 0.01326864 training_loss 9.914937sec time\n","24 epoch, 0.01577872 valid_loss, 0.01317756 training_loss 10.115987sec time\n","Epoch    24: reducing learning rate of group 0 to 7.8097e-05.\n","completed training one fold -------------- \n","1 epoch, 0.01986003 valid_loss, 0.05880746 training_loss 9.349326sec time\n","2 epoch, 0.01864178 valid_loss, 0.01881817 training_loss 9.308794sec time\n","3 epoch, 0.01789803 valid_loss, 0.01791406 training_loss 9.378377sec time\n","4 epoch, 0.01701492 valid_loss, 0.01701458 training_loss 9.318418sec time\n","5 epoch, 0.01657562 valid_loss, 0.01638685 training_loss 9.327308sec time\n","6 epoch, 0.01630990 valid_loss, 0.01606545 training_loss 9.359848sec time\n","7 epoch, 0.01639477 valid_loss, 0.01572778 training_loss 9.492102sec time\n","8 epoch, 0.01604070 valid_loss, 0.01552902 training_loss 9.209484sec time\n","9 epoch, 0.01603083 valid_loss, 0.01544601 training_loss 9.320124sec time\n","10 epoch, 0.01612677 valid_loss, 0.01522770 training_loss 9.347141sec time\n","11 epoch, 0.01596065 valid_loss, 0.01509049 training_loss 9.335501sec time\n","12 epoch, 0.01591740 valid_loss, 0.01504325 training_loss 9.426155sec time\n","13 epoch, 0.01572281 valid_loss, 0.01489811 training_loss 9.398988sec time\n","14 epoch, 0.01581845 valid_loss, 0.01482919 training_loss 9.392552sec time\n","15 epoch, 0.01594315 valid_loss, 0.01480948 training_loss 9.511984sec time\n","16 epoch, 0.01587781 valid_loss, 0.01481608 training_loss 10.177501sec time\n","17 epoch, 0.01601528 valid_loss, 0.01472478 training_loss 12.065686sec time\n","Epoch    17: reducing learning rate of group 0 to 7.8097e-04.\n","18 epoch, 0.01540582 valid_loss, 0.01412119 training_loss 13.079215sec time\n","19 epoch, 0.01542491 valid_loss, 0.01373819 training_loss 13.177601sec time\n","20 epoch, 0.01548588 valid_loss, 0.01357963 training_loss 13.232579sec time\n","21 epoch, 0.01552702 valid_loss, 0.01344598 training_loss 13.434776sec time\n","22 epoch, 0.01545628 valid_loss, 0.01333411 training_loss 13.666439sec time\n","Epoch    22: reducing learning rate of group 0 to 7.8097e-05.\n","23 epoch, 0.01546728 valid_loss, 0.01307752 training_loss 13.819609sec time\n","24 epoch, 0.01548999 valid_loss, 0.01301200 training_loss 13.934358sec time\n","completed training one fold -------------- \n","1 epoch, 0.01896935 valid_loss, 0.05943721 training_loss 9.149870sec time\n","2 epoch, 0.01830416 valid_loss, 0.01891129 training_loss 9.321354sec time\n","3 epoch, 0.01785157 valid_loss, 0.01815919 training_loss 9.323560sec time\n","4 epoch, 0.01707618 valid_loss, 0.01738477 training_loss 9.255114sec time\n","5 epoch, 0.01659517 valid_loss, 0.01666413 training_loss 9.263350sec time\n","6 epoch, 0.01651892 valid_loss, 0.01617138 training_loss 9.282552sec time\n","7 epoch, 0.01616194 valid_loss, 0.01589543 training_loss 9.223037sec time\n","8 epoch, 0.01616617 valid_loss, 0.01565878 training_loss 9.250230sec time\n","9 epoch, 0.01598425 valid_loss, 0.01543686 training_loss 9.225196sec time\n","10 epoch, 0.01593487 valid_loss, 0.01538656 training_loss 9.263656sec time\n","11 epoch, 0.01582803 valid_loss, 0.01519117 training_loss 9.395164sec time\n","12 epoch, 0.01563503 valid_loss, 0.01506293 training_loss 9.337155sec time\n","13 epoch, 0.01603269 valid_loss, 0.01500494 training_loss 9.355442sec time\n","14 epoch, 0.01575585 valid_loss, 0.01499479 training_loss 9.286875sec time\n","15 epoch, 0.01574760 valid_loss, 0.01484005 training_loss 9.256550sec time\n","16 epoch, 0.01557247 valid_loss, 0.01493395 training_loss 9.524229sec time\n","17 epoch, 0.01574784 valid_loss, 0.01468304 training_loss 10.155765sec time\n","18 epoch, 0.01573037 valid_loss, 0.01477786 training_loss 11.168392sec time\n","19 epoch, 0.01571145 valid_loss, 0.01470428 training_loss 13.171154sec time\n","20 epoch, 0.01544507 valid_loss, 0.01465434 training_loss 15.164431sec time\n","21 epoch, 0.01562595 valid_loss, 0.01461996 training_loss 17.070453sec time\n","22 epoch, 0.01568774 valid_loss, 0.01462952 training_loss 19.682396sec time\n","23 epoch, 0.01560525 valid_loss, 0.01458468 training_loss 21.450908sec time\n","24 epoch, 0.01552610 valid_loss, 0.01453432 training_loss 24.195954sec time\n","Epoch    24: reducing learning rate of group 0 to 7.8097e-04.\n","completed training one fold -------------- \n","1 epoch, 0.01955431 valid_loss, 0.06052144 training_loss 9.310545sec time\n","2 epoch, 0.01862123 valid_loss, 0.01903114 training_loss 9.224626sec time\n","3 epoch, 0.01799201 valid_loss, 0.01825524 training_loss 9.225340sec time\n","4 epoch, 0.01717108 valid_loss, 0.01739573 training_loss 9.184071sec time\n","5 epoch, 0.01664399 valid_loss, 0.01666997 training_loss 9.300792sec time\n","6 epoch, 0.01653190 valid_loss, 0.01620289 training_loss 9.205058sec time\n","7 epoch, 0.01611855 valid_loss, 0.01595699 training_loss 9.213664sec time\n","8 epoch, 0.01611170 valid_loss, 0.01567786 training_loss 9.240257sec time\n","9 epoch, 0.01571929 valid_loss, 0.01554374 training_loss 9.146717sec time\n","10 epoch, 0.01559236 valid_loss, 0.01528928 training_loss 9.135669sec time\n","11 epoch, 0.01590584 valid_loss, 0.01517714 training_loss 9.234340sec time\n","12 epoch, 0.01545434 valid_loss, 0.01510961 training_loss 9.285958sec time\n","13 epoch, 0.01542844 valid_loss, 0.01500375 training_loss 9.395461sec time\n","14 epoch, 0.01542883 valid_loss, 0.01500481 training_loss 9.385746sec time\n","15 epoch, 0.01530035 valid_loss, 0.01488818 training_loss 9.333736sec time\n","16 epoch, 0.01549101 valid_loss, 0.01488884 training_loss 9.475569sec time\n","17 epoch, 0.01532488 valid_loss, 0.01482270 training_loss 9.866898sec time\n","18 epoch, 0.01549783 valid_loss, 0.01477087 training_loss 10.657806sec time\n","19 epoch, 0.01526915 valid_loss, 0.01484331 training_loss 11.222693sec time\n","20 epoch, 0.01550544 valid_loss, 0.01471882 training_loss 12.509179sec time\n","21 epoch, 0.01519962 valid_loss, 0.01477752 training_loss 14.674222sec time\n","22 epoch, 0.01545390 valid_loss, 0.01469844 training_loss 18.658169sec time\n","23 epoch, 0.01523095 valid_loss, 0.01470057 training_loss 21.859896sec time\n","24 epoch, 0.01527463 valid_loss, 0.01457464 training_loss 23.847788sec time\n","completed training one fold -------------- \n"]}]},{"cell_type":"code","source":["losses"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IBrDme1KQNNp","executionInfo":{"status":"ok","timestamp":1642562985175,"user_tz":-480,"elapsed":48,"user":{"displayName":"Indrik Wijaya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00208067985282844199"}},"outputId":"252d8040-0048-4b58-839d-5c555de12f20"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.01955430627557911,\n"," 0.01862123093935622,\n"," 0.017992009635732956,\n"," 0.017171083108968095,\n"," 0.016643987339334327,\n"," 0.016531902144686514,\n"," 0.016118551026873228,\n"," 0.01611169914738471,\n"," 0.015719288448263116,\n"," 0.015592357763597945,\n"," 0.015905841045519883,\n"," 0.015454335668820793,\n"," 0.015428442558070191,\n"," 0.01542883472783225,\n"," 0.01530035108703525,\n"," 0.015491006911552254,\n"," 0.015324883094104399,\n"," 0.015497832480786728,\n"," 0.015269148376371179,\n"," 0.015505443225387766,\n"," 0.01519961832639049,\n"," 0.015453900891442258,\n"," 0.015230949244955007,\n"," 0.01527463062940275]"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["valid_losses"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jA5MVE1rMKi8","executionInfo":{"status":"ok","timestamp":1642562985177,"user_tz":-480,"elapsed":26,"user":{"displayName":"Indrik Wijaya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00208067985282844199"}},"outputId":"246bb770-7b57-4c35-f222-b48617d53e2d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.01955430627557911,\n"," 0.01862123093935622,\n"," 0.017992009635732956,\n"," 0.017171083108968095,\n"," 0.016643987339334327,\n"," 0.016531902144686514,\n"," 0.016118551026873228,\n"," 0.01611169914738471,\n"," 0.015719288448263116,\n"," 0.015592357763597945,\n"," 0.015905841045519883,\n"," 0.015454335668820793,\n"," 0.015428442558070191,\n"," 0.01542883472783225,\n"," 0.01530035108703525,\n"," 0.015491006911552254,\n"," 0.015324883094104399,\n"," 0.015497832480786728,\n"," 0.015269148376371179,\n"," 0.015505443225387766,\n"," 0.01519961832639049,\n"," 0.015453900891442258,\n"," 0.015230949244955007,\n"," 0.01527463062940275]"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":[""],"metadata":{"id":"KEWrmx0gML-v"},"execution_count":null,"outputs":[]}]}