{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4. DNN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNSC5SiuMK9UKEkRAu9OEJi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9KtHiBHJ3YPM"},"source":["This notebook is developed from [Kaggle's Competition](https://www.kaggle.com/c/lish-moa) on prediction drugs' mechanisms of action.\n","\n","Some Kaggle's notebook inspirations: \n","\n","1) [Oleksandr Sirenko](https://github.com/oleksandrsirenko/mechanisms-of-action-moa-prediction#inbox_tray-how-to-get-data)\n","\n","2) [Kushal Agrawal](https://www.kaggle.com/kushal1506/moa-prediction-complete-walkthrough-eda-ensemble/notebook)\n","\n","3) [Aavajanar](https://github.com/aavajanar/KAGGLE-MOA)\n","\n","# Background\n","The Connectivity Map, a project within the Broad Institute of MIT and Harvard, the Laboratory for Innovation Science at Harvard (LISH), and the NIH Common Funds Library of Integrated Network-Based Cellular Signatures (LINCS), present this challenge with the goal of advancing drug development through improvements to MoA prediction algorithms.\n","\n","Task: Predict multiple targets of the Mechanism of Action (MoA) response(s) of different samples (sig_id), given various inputs such as gene expression data and cell viability data\n","\n","Two notes:\n","- the training data has an optional set of MoA labels that aren't included in the test data and not used for scoring\n","- the re-run dataset has approximately 4x the number of examples seen in the Public test\n","\n","## What is the Mechanism of Action (MoA) of a drug? And why is it important?\n","In the past, scientists derived drugs from natural products or were inspired by traditional remedies. Very common drugs, such as paracetamol, known in the US as acetaminophen, were put into clinical use decades before the biological mechanisms driving their pharmacological activities were understood. Today, with the advent of more powerful technologies, drug discovery has changed from the serendipitous approaches of the past to a more targeted model based on an understanding of the underlying biological mechanisms of a disease. In this new framework, scientists seek to identify a protein target associated w/ a disease and develop a molecule that can modulate that protein target. As a shorthand to describe the biological activitiy of a given molecule, scientists assign a label referred to as mechanism-of-action of MoA for short.\n","\n","## How do we determine the MoAs of a new drug?\n","One approach is to treat a sample of human cells with the drug and then analyze the cellular responses with algorithms that search for similarity to known patterns in large genomic databases, such as libraries of gene expression or cell viability patterns of drugs with known MoAs.\n","\n","The data combine gene expression and cell viability data, is based on a new technology that measures simulatenously (within the same samples) human cells' responses to drugs in a pool of 100 different cell types (thus solving the problm of identifying ex-ante, which cell types are better suited for a given drug). In addition, you will have access to MoA annotations for > 5,000 drugs in this dataset.\n","\n","## How to evaluate the accuracy of a solution?\n","Based on the MoA annotations, the accuracy of solutions will be evaluated on the average value of the logarithmic loss function applied to each drug-MoA annotation pair.\n","\n","## Data\n","- `train_features.csv` - Features for the training set. Features g- signify gene expression data, and c- signify cell viability data. cp_type indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); control perturbations have no MoAs; cp_time and cp_dose indicate treatment duration (24, 48, 72 hours) and dose (high or low).\n","- `train_targets_scored.csv` - The binary MoA targets that are scored.\n","- `train_targets_nonscored.csv` - Additional (optional) binary MoA responses for the training data. These are not predicted nor scored.\n","- `test_features.csv` - Features for the test data. You must predict the probability of each scored MoA for each row in the test data.\n","- `sample_submission.csv` - A submission file in the correct format."]},{"cell_type":"code","metadata":{"id":"izp_7gW581ay","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642411681498,"user_tz":-480,"elapsed":3567,"user":{"displayName":"Indrik Wijaya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00208067985282844199"}},"outputId":"b3bed6b5-14d2-41d9-d5d1-9ce02f8c83a2"},"source":["! pip install tensorflow-addons\n","#! unzip lish-moa.zip"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.7/dist-packages (0.15.0)\n","Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n"]}]},{"cell_type":"code","metadata":{"id":"35Pw9Rc37Wdj"},"source":["import numpy as np \n","import pandas as pd \n","import os\n","\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import log_loss\n"," \n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","import tensorflow.keras.backend as K\n","import tensorflow.keras.layers as L\n","import tensorflow.keras.models as M\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y1LelozW9ADt"},"source":["# Load datasets, try first 1000 lines for example\n","test_df = pd.read_csv('test_features.csv')\n","train_df = pd.read_csv('train_features.csv')\n","train_target_df = pd.read_csv('train_targets_scored.csv')\n","sub = pd.read_csv('sample_submission.csv')\n","\n","target_cols = train_target_df.columns[1:]\n","N_TARGETS = len(target_cols)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TW9ovZLz-Pfi"},"source":["test_df = test_df.iloc[0:10000:,]\n","train_df = train_df.iloc[0:10000,:]\n","sub = sub.iloc[0:10000,:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"M7D1PUPI-VZ0","executionInfo":{"status":"ok","timestamp":1642411686879,"user_tz":-480,"elapsed":765,"user":{"displayName":"Indrik Wijaya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00208067985282844199"}},"outputId":"13775ae1-9f9d-4bcf-b53c-f9ae2ffdc0b0"},"source":["train_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-284cb399-1978-4d17-a59f-532608263cec\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sig_id</th>\n","      <th>cp_type</th>\n","      <th>cp_time</th>\n","      <th>cp_dose</th>\n","      <th>g-0</th>\n","      <th>g-1</th>\n","      <th>g-2</th>\n","      <th>g-3</th>\n","      <th>g-4</th>\n","      <th>g-5</th>\n","      <th>g-6</th>\n","      <th>g-7</th>\n","      <th>g-8</th>\n","      <th>g-9</th>\n","      <th>g-10</th>\n","      <th>g-11</th>\n","      <th>g-12</th>\n","      <th>g-13</th>\n","      <th>g-14</th>\n","      <th>g-15</th>\n","      <th>g-16</th>\n","      <th>g-17</th>\n","      <th>g-18</th>\n","      <th>g-19</th>\n","      <th>g-20</th>\n","      <th>g-21</th>\n","      <th>g-22</th>\n","      <th>g-23</th>\n","      <th>g-24</th>\n","      <th>g-25</th>\n","      <th>g-26</th>\n","      <th>g-27</th>\n","      <th>g-28</th>\n","      <th>g-29</th>\n","      <th>g-30</th>\n","      <th>g-31</th>\n","      <th>g-32</th>\n","      <th>g-33</th>\n","      <th>g-34</th>\n","      <th>g-35</th>\n","      <th>...</th>\n","      <th>c-60</th>\n","      <th>c-61</th>\n","      <th>c-62</th>\n","      <th>c-63</th>\n","      <th>c-64</th>\n","      <th>c-65</th>\n","      <th>c-66</th>\n","      <th>c-67</th>\n","      <th>c-68</th>\n","      <th>c-69</th>\n","      <th>c-70</th>\n","      <th>c-71</th>\n","      <th>c-72</th>\n","      <th>c-73</th>\n","      <th>c-74</th>\n","      <th>c-75</th>\n","      <th>c-76</th>\n","      <th>c-77</th>\n","      <th>c-78</th>\n","      <th>c-79</th>\n","      <th>c-80</th>\n","      <th>c-81</th>\n","      <th>c-82</th>\n","      <th>c-83</th>\n","      <th>c-84</th>\n","      <th>c-85</th>\n","      <th>c-86</th>\n","      <th>c-87</th>\n","      <th>c-88</th>\n","      <th>c-89</th>\n","      <th>c-90</th>\n","      <th>c-91</th>\n","      <th>c-92</th>\n","      <th>c-93</th>\n","      <th>c-94</th>\n","      <th>c-95</th>\n","      <th>c-96</th>\n","      <th>c-97</th>\n","      <th>c-98</th>\n","      <th>c-99</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>id_000644bb2</td>\n","      <td>trt_cp</td>\n","      <td>24</td>\n","      <td>D1</td>\n","      <td>1.0620</td>\n","      <td>0.5577</td>\n","      <td>-0.2479</td>\n","      <td>-0.6208</td>\n","      <td>-0.1944</td>\n","      <td>-1.0120</td>\n","      <td>-1.0220</td>\n","      <td>-0.0326</td>\n","      <td>0.5548</td>\n","      <td>-0.0921</td>\n","      <td>1.1830</td>\n","      <td>0.1530</td>\n","      <td>0.5574</td>\n","      <td>-0.4015</td>\n","      <td>0.1789</td>\n","      <td>-0.6528</td>\n","      <td>-0.7969</td>\n","      <td>0.6342</td>\n","      <td>0.1778</td>\n","      <td>-0.3694</td>\n","      <td>-0.5688</td>\n","      <td>-1.1360</td>\n","      <td>-1.1880</td>\n","      <td>0.6940</td>\n","      <td>0.4393</td>\n","      <td>0.2664</td>\n","      <td>0.1907</td>\n","      <td>0.1628</td>\n","      <td>-0.2853</td>\n","      <td>0.5819</td>\n","      <td>0.2934</td>\n","      <td>-0.5584</td>\n","      <td>-0.0916</td>\n","      <td>-0.3010</td>\n","      <td>-0.1537</td>\n","      <td>0.2198</td>\n","      <td>...</td>\n","      <td>0.4805</td>\n","      <td>0.4965</td>\n","      <td>0.3680</td>\n","      <td>0.8427</td>\n","      <td>0.1042</td>\n","      <td>0.1403</td>\n","      <td>0.1758</td>\n","      <td>1.2570</td>\n","      <td>-0.5979</td>\n","      <td>1.2250</td>\n","      <td>-0.0553</td>\n","      <td>0.7351</td>\n","      <td>0.5810</td>\n","      <td>0.9590</td>\n","      <td>0.2427</td>\n","      <td>0.0495</td>\n","      <td>0.4141</td>\n","      <td>0.8432</td>\n","      <td>0.6162</td>\n","      <td>-0.7318</td>\n","      <td>1.2120</td>\n","      <td>0.6362</td>\n","      <td>-0.4427</td>\n","      <td>0.1288</td>\n","      <td>1.4840</td>\n","      <td>0.1799</td>\n","      <td>0.5367</td>\n","      <td>-0.1111</td>\n","      <td>-1.0120</td>\n","      <td>0.6685</td>\n","      <td>0.2862</td>\n","      <td>0.2584</td>\n","      <td>0.8076</td>\n","      <td>0.5523</td>\n","      <td>-0.1912</td>\n","      <td>0.6584</td>\n","      <td>-0.3981</td>\n","      <td>0.2139</td>\n","      <td>0.3801</td>\n","      <td>0.4176</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>id_000779bfc</td>\n","      <td>trt_cp</td>\n","      <td>72</td>\n","      <td>D1</td>\n","      <td>0.0743</td>\n","      <td>0.4087</td>\n","      <td>0.2991</td>\n","      <td>0.0604</td>\n","      <td>1.0190</td>\n","      <td>0.5207</td>\n","      <td>0.2341</td>\n","      <td>0.3372</td>\n","      <td>-0.4047</td>\n","      <td>0.8507</td>\n","      <td>-1.1520</td>\n","      <td>-0.4201</td>\n","      <td>-0.0958</td>\n","      <td>0.4590</td>\n","      <td>0.0803</td>\n","      <td>0.2250</td>\n","      <td>0.5293</td>\n","      <td>0.2839</td>\n","      <td>-0.3494</td>\n","      <td>0.2883</td>\n","      <td>0.9449</td>\n","      <td>-0.1646</td>\n","      <td>-0.2657</td>\n","      <td>-0.3372</td>\n","      <td>0.3135</td>\n","      <td>-0.4316</td>\n","      <td>0.4773</td>\n","      <td>0.2075</td>\n","      <td>-0.4216</td>\n","      <td>-0.1161</td>\n","      <td>-0.0499</td>\n","      <td>-0.2627</td>\n","      <td>0.9959</td>\n","      <td>-0.2483</td>\n","      <td>0.2655</td>\n","      <td>-0.2102</td>\n","      <td>...</td>\n","      <td>0.4083</td>\n","      <td>0.0319</td>\n","      <td>0.3905</td>\n","      <td>0.7099</td>\n","      <td>0.2912</td>\n","      <td>0.4151</td>\n","      <td>-0.2840</td>\n","      <td>-0.3104</td>\n","      <td>-0.6373</td>\n","      <td>0.2887</td>\n","      <td>-0.0765</td>\n","      <td>0.2539</td>\n","      <td>0.4443</td>\n","      <td>0.5932</td>\n","      <td>0.2031</td>\n","      <td>0.7639</td>\n","      <td>0.5499</td>\n","      <td>-0.3322</td>\n","      <td>-0.0977</td>\n","      <td>0.4329</td>\n","      <td>-0.2782</td>\n","      <td>0.7827</td>\n","      <td>0.5934</td>\n","      <td>0.3402</td>\n","      <td>0.1499</td>\n","      <td>0.4420</td>\n","      <td>0.9366</td>\n","      <td>0.8193</td>\n","      <td>-0.4236</td>\n","      <td>0.3192</td>\n","      <td>-0.4265</td>\n","      <td>0.7543</td>\n","      <td>0.4708</td>\n","      <td>0.0230</td>\n","      <td>0.2957</td>\n","      <td>0.4899</td>\n","      <td>0.1522</td>\n","      <td>0.1241</td>\n","      <td>0.6077</td>\n","      <td>0.7371</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>id_000a6266a</td>\n","      <td>trt_cp</td>\n","      <td>48</td>\n","      <td>D1</td>\n","      <td>0.6280</td>\n","      <td>0.5817</td>\n","      <td>1.5540</td>\n","      <td>-0.0764</td>\n","      <td>-0.0323</td>\n","      <td>1.2390</td>\n","      <td>0.1715</td>\n","      <td>0.2155</td>\n","      <td>0.0065</td>\n","      <td>1.2300</td>\n","      <td>-0.4797</td>\n","      <td>-0.5631</td>\n","      <td>-0.0366</td>\n","      <td>-1.8300</td>\n","      <td>0.6057</td>\n","      <td>-0.3278</td>\n","      <td>0.6042</td>\n","      <td>-0.3075</td>\n","      <td>-0.1147</td>\n","      <td>-0.0570</td>\n","      <td>-0.0799</td>\n","      <td>-0.8181</td>\n","      <td>-1.5320</td>\n","      <td>0.2307</td>\n","      <td>0.4901</td>\n","      <td>0.4780</td>\n","      <td>-1.3970</td>\n","      <td>4.6240</td>\n","      <td>-0.0437</td>\n","      <td>1.2870</td>\n","      <td>-1.8530</td>\n","      <td>0.6069</td>\n","      <td>0.4290</td>\n","      <td>0.1783</td>\n","      <td>0.0018</td>\n","      <td>-1.1800</td>\n","      <td>...</td>\n","      <td>-0.5477</td>\n","      <td>-0.7576</td>\n","      <td>-0.0444</td>\n","      <td>0.1894</td>\n","      <td>-0.0014</td>\n","      <td>-2.3640</td>\n","      <td>-0.4682</td>\n","      <td>0.1210</td>\n","      <td>-0.5177</td>\n","      <td>-0.0604</td>\n","      <td>0.1682</td>\n","      <td>-0.4436</td>\n","      <td>0.4963</td>\n","      <td>0.1363</td>\n","      <td>0.3335</td>\n","      <td>0.9760</td>\n","      <td>-0.0427</td>\n","      <td>-0.1235</td>\n","      <td>0.0959</td>\n","      <td>0.0690</td>\n","      <td>-0.9416</td>\n","      <td>-0.7548</td>\n","      <td>-0.1109</td>\n","      <td>-0.6272</td>\n","      <td>0.3019</td>\n","      <td>0.1172</td>\n","      <td>0.1093</td>\n","      <td>-0.3113</td>\n","      <td>0.3019</td>\n","      <td>-0.0873</td>\n","      <td>-0.7250</td>\n","      <td>-0.6297</td>\n","      <td>0.6103</td>\n","      <td>0.0223</td>\n","      <td>-1.3240</td>\n","      <td>-0.3174</td>\n","      <td>-0.6417</td>\n","      <td>-0.2187</td>\n","      <td>-1.4080</td>\n","      <td>0.6931</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>id_0015fd391</td>\n","      <td>trt_cp</td>\n","      <td>48</td>\n","      <td>D1</td>\n","      <td>-0.5138</td>\n","      <td>-0.2491</td>\n","      <td>-0.2656</td>\n","      <td>0.5288</td>\n","      <td>4.0620</td>\n","      <td>-0.8095</td>\n","      <td>-1.9590</td>\n","      <td>0.1792</td>\n","      <td>-0.1321</td>\n","      <td>-1.0600</td>\n","      <td>-0.8269</td>\n","      <td>-0.3584</td>\n","      <td>-0.8511</td>\n","      <td>-0.5844</td>\n","      <td>-2.5690</td>\n","      <td>0.8183</td>\n","      <td>-0.0532</td>\n","      <td>-0.8554</td>\n","      <td>0.1160</td>\n","      <td>-2.3520</td>\n","      <td>2.1200</td>\n","      <td>-1.1580</td>\n","      <td>-0.7191</td>\n","      <td>-0.8004</td>\n","      <td>-1.4670</td>\n","      <td>-0.0107</td>\n","      <td>-0.8995</td>\n","      <td>0.2406</td>\n","      <td>-0.2479</td>\n","      <td>-1.0890</td>\n","      <td>-0.7575</td>\n","      <td>0.0881</td>\n","      <td>-2.7370</td>\n","      <td>0.8745</td>\n","      <td>0.5787</td>\n","      <td>-1.6740</td>\n","      <td>...</td>\n","      <td>-2.1220</td>\n","      <td>-0.3752</td>\n","      <td>-2.3820</td>\n","      <td>-3.7350</td>\n","      <td>-2.9740</td>\n","      <td>-1.4930</td>\n","      <td>-1.6600</td>\n","      <td>-3.1660</td>\n","      <td>0.2816</td>\n","      <td>-0.2990</td>\n","      <td>-1.1870</td>\n","      <td>-0.5044</td>\n","      <td>-1.7750</td>\n","      <td>-1.6120</td>\n","      <td>-0.9215</td>\n","      <td>-1.0810</td>\n","      <td>-3.0520</td>\n","      <td>-3.4470</td>\n","      <td>-2.7740</td>\n","      <td>-1.8460</td>\n","      <td>-0.5568</td>\n","      <td>-3.3960</td>\n","      <td>-2.9510</td>\n","      <td>-1.1550</td>\n","      <td>-3.2620</td>\n","      <td>-1.5390</td>\n","      <td>-2.4600</td>\n","      <td>-0.9417</td>\n","      <td>-1.5550</td>\n","      <td>0.2431</td>\n","      <td>-2.0990</td>\n","      <td>-0.6441</td>\n","      <td>-5.6300</td>\n","      <td>-1.3780</td>\n","      <td>-0.8632</td>\n","      <td>-1.2880</td>\n","      <td>-1.6210</td>\n","      <td>-0.8784</td>\n","      <td>-0.3876</td>\n","      <td>-0.8154</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>id_001626bd3</td>\n","      <td>trt_cp</td>\n","      <td>72</td>\n","      <td>D2</td>\n","      <td>-0.3254</td>\n","      <td>-0.4009</td>\n","      <td>0.9700</td>\n","      <td>0.6919</td>\n","      <td>1.4180</td>\n","      <td>-0.8244</td>\n","      <td>-0.2800</td>\n","      <td>-0.1498</td>\n","      <td>-0.8789</td>\n","      <td>0.8630</td>\n","      <td>-0.2219</td>\n","      <td>-0.5121</td>\n","      <td>-0.9577</td>\n","      <td>1.1750</td>\n","      <td>0.2042</td>\n","      <td>0.1970</td>\n","      <td>0.1244</td>\n","      <td>-1.7090</td>\n","      <td>-0.3543</td>\n","      <td>-0.5160</td>\n","      <td>-0.3330</td>\n","      <td>-0.2685</td>\n","      <td>0.7649</td>\n","      <td>0.2057</td>\n","      <td>1.3720</td>\n","      <td>0.6835</td>\n","      <td>0.8056</td>\n","      <td>-0.3754</td>\n","      <td>-1.2090</td>\n","      <td>0.2965</td>\n","      <td>-0.0712</td>\n","      <td>0.6389</td>\n","      <td>0.6674</td>\n","      <td>-0.0783</td>\n","      <td>1.1740</td>\n","      <td>-0.7110</td>\n","      <td>...</td>\n","      <td>-0.2274</td>\n","      <td>0.3215</td>\n","      <td>0.1535</td>\n","      <td>-0.4640</td>\n","      <td>-0.5943</td>\n","      <td>0.3973</td>\n","      <td>0.1500</td>\n","      <td>0.5178</td>\n","      <td>0.5159</td>\n","      <td>0.6091</td>\n","      <td>0.1813</td>\n","      <td>-0.4249</td>\n","      <td>0.7832</td>\n","      <td>0.6529</td>\n","      <td>0.5648</td>\n","      <td>0.4817</td>\n","      <td>0.0587</td>\n","      <td>0.5303</td>\n","      <td>0.6376</td>\n","      <td>-0.3966</td>\n","      <td>-1.4950</td>\n","      <td>-0.9625</td>\n","      <td>-0.0541</td>\n","      <td>0.6273</td>\n","      <td>0.4563</td>\n","      <td>0.0698</td>\n","      <td>0.8134</td>\n","      <td>0.1924</td>\n","      <td>0.6054</td>\n","      <td>-0.1824</td>\n","      <td>0.0042</td>\n","      <td>0.0048</td>\n","      <td>0.6670</td>\n","      <td>1.0690</td>\n","      <td>0.5523</td>\n","      <td>-0.3031</td>\n","      <td>0.1094</td>\n","      <td>0.2885</td>\n","      <td>-0.3786</td>\n","      <td>0.7125</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 876 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-284cb399-1978-4d17-a59f-532608263cec')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-284cb399-1978-4d17-a59f-532608263cec button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-284cb399-1978-4d17-a59f-532608263cec');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["         sig_id cp_type  cp_time cp_dose  ...    c-96    c-97    c-98    c-99\n","0  id_000644bb2  trt_cp       24      D1  ... -0.3981  0.2139  0.3801  0.4176\n","1  id_000779bfc  trt_cp       72      D1  ...  0.1522  0.1241  0.6077  0.7371\n","2  id_000a6266a  trt_cp       48      D1  ... -0.6417 -0.2187 -1.4080  0.6931\n","3  id_0015fd391  trt_cp       48      D1  ... -1.6210 -0.8784 -0.3876 -0.8154\n","4  id_001626bd3  trt_cp       72      D2  ...  0.1094  0.2885 -0.3786  0.7125\n","\n","[5 rows x 876 columns]"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"a4be0T-H-XGh","executionInfo":{"status":"ok","timestamp":1642411686881,"user_tz":-480,"elapsed":20,"user":{"displayName":"Indrik Wijaya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00208067985282844199"}},"outputId":"fa0865ca-da04-48f5-9851-88918a0fc13b"},"source":["test_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-e5be4907-b8a6-4226-90cb-1a58bb602f32\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sig_id</th>\n","      <th>cp_type</th>\n","      <th>cp_time</th>\n","      <th>cp_dose</th>\n","      <th>g-0</th>\n","      <th>g-1</th>\n","      <th>g-2</th>\n","      <th>g-3</th>\n","      <th>g-4</th>\n","      <th>g-5</th>\n","      <th>g-6</th>\n","      <th>g-7</th>\n","      <th>g-8</th>\n","      <th>g-9</th>\n","      <th>g-10</th>\n","      <th>g-11</th>\n","      <th>g-12</th>\n","      <th>g-13</th>\n","      <th>g-14</th>\n","      <th>g-15</th>\n","      <th>g-16</th>\n","      <th>g-17</th>\n","      <th>g-18</th>\n","      <th>g-19</th>\n","      <th>g-20</th>\n","      <th>g-21</th>\n","      <th>g-22</th>\n","      <th>g-23</th>\n","      <th>g-24</th>\n","      <th>g-25</th>\n","      <th>g-26</th>\n","      <th>g-27</th>\n","      <th>g-28</th>\n","      <th>g-29</th>\n","      <th>g-30</th>\n","      <th>g-31</th>\n","      <th>g-32</th>\n","      <th>g-33</th>\n","      <th>g-34</th>\n","      <th>g-35</th>\n","      <th>...</th>\n","      <th>c-60</th>\n","      <th>c-61</th>\n","      <th>c-62</th>\n","      <th>c-63</th>\n","      <th>c-64</th>\n","      <th>c-65</th>\n","      <th>c-66</th>\n","      <th>c-67</th>\n","      <th>c-68</th>\n","      <th>c-69</th>\n","      <th>c-70</th>\n","      <th>c-71</th>\n","      <th>c-72</th>\n","      <th>c-73</th>\n","      <th>c-74</th>\n","      <th>c-75</th>\n","      <th>c-76</th>\n","      <th>c-77</th>\n","      <th>c-78</th>\n","      <th>c-79</th>\n","      <th>c-80</th>\n","      <th>c-81</th>\n","      <th>c-82</th>\n","      <th>c-83</th>\n","      <th>c-84</th>\n","      <th>c-85</th>\n","      <th>c-86</th>\n","      <th>c-87</th>\n","      <th>c-88</th>\n","      <th>c-89</th>\n","      <th>c-90</th>\n","      <th>c-91</th>\n","      <th>c-92</th>\n","      <th>c-93</th>\n","      <th>c-94</th>\n","      <th>c-95</th>\n","      <th>c-96</th>\n","      <th>c-97</th>\n","      <th>c-98</th>\n","      <th>c-99</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>id_0004d9e33</td>\n","      <td>trt_cp</td>\n","      <td>24</td>\n","      <td>D1</td>\n","      <td>-0.5458</td>\n","      <td>0.1306</td>\n","      <td>-0.5135</td>\n","      <td>0.4408</td>\n","      <td>1.5500</td>\n","      <td>-0.1644</td>\n","      <td>-0.2140</td>\n","      <td>0.2221</td>\n","      <td>-0.3260</td>\n","      <td>1.9390</td>\n","      <td>-0.2305</td>\n","      <td>-0.3670</td>\n","      <td>1.3040</td>\n","      <td>1.4610</td>\n","      <td>0.0043</td>\n","      <td>0.6816</td>\n","      <td>-0.2304</td>\n","      <td>-0.0635</td>\n","      <td>-0.2030</td>\n","      <td>-0.6821</td>\n","      <td>-0.6242</td>\n","      <td>0.1297</td>\n","      <td>-0.0338</td>\n","      <td>0.3372</td>\n","      <td>0.2254</td>\n","      <td>0.4795</td>\n","      <td>0.7642</td>\n","      <td>0.6638</td>\n","      <td>-0.2480</td>\n","      <td>-0.1183</td>\n","      <td>-0.4847</td>\n","      <td>-0.0179</td>\n","      <td>-0.8204</td>\n","      <td>-0.5296</td>\n","      <td>-1.5070</td>\n","      <td>-0.0144</td>\n","      <td>...</td>\n","      <td>-0.1353</td>\n","      <td>0.0494</td>\n","      <td>0.8939</td>\n","      <td>0.2270</td>\n","      <td>0.2876</td>\n","      <td>-0.3065</td>\n","      <td>0.6519</td>\n","      <td>-0.8156</td>\n","      <td>-1.4960</td>\n","      <td>0.3796</td>\n","      <td>0.0877</td>\n","      <td>-1.0230</td>\n","      <td>-0.0206</td>\n","      <td>-0.4149</td>\n","      <td>-0.6258</td>\n","      <td>-0.2688</td>\n","      <td>0.4403</td>\n","      <td>-0.4900</td>\n","      <td>0.2910</td>\n","      <td>0.0473</td>\n","      <td>-0.0914</td>\n","      <td>0.3087</td>\n","      <td>-0.0612</td>\n","      <td>-0.9128</td>\n","      <td>-0.9399</td>\n","      <td>0.0173</td>\n","      <td>0.0519</td>\n","      <td>-0.0035</td>\n","      <td>-0.5184</td>\n","      <td>-0.3485</td>\n","      <td>0.0981</td>\n","      <td>0.7978</td>\n","      <td>-0.1430</td>\n","      <td>-0.2067</td>\n","      <td>-0.2303</td>\n","      <td>-0.1193</td>\n","      <td>0.0210</td>\n","      <td>-0.0502</td>\n","      <td>0.1510</td>\n","      <td>-0.7750</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>id_001897cda</td>\n","      <td>trt_cp</td>\n","      <td>72</td>\n","      <td>D1</td>\n","      <td>-0.1829</td>\n","      <td>0.2320</td>\n","      <td>1.2080</td>\n","      <td>-0.4522</td>\n","      <td>-0.3652</td>\n","      <td>-0.3319</td>\n","      <td>-1.8820</td>\n","      <td>0.4022</td>\n","      <td>-0.3528</td>\n","      <td>0.1271</td>\n","      <td>0.9303</td>\n","      <td>0.3173</td>\n","      <td>-1.0120</td>\n","      <td>-0.3213</td>\n","      <td>0.0607</td>\n","      <td>-0.5389</td>\n","      <td>-0.8030</td>\n","      <td>-1.0600</td>\n","      <td>-0.0978</td>\n","      <td>-0.8156</td>\n","      <td>-0.6514</td>\n","      <td>0.6812</td>\n","      <td>0.5246</td>\n","      <td>0.0000</td>\n","      <td>0.5030</td>\n","      <td>-0.1500</td>\n","      <td>-0.1433</td>\n","      <td>2.0910</td>\n","      <td>-0.6556</td>\n","      <td>-0.6012</td>\n","      <td>-0.4104</td>\n","      <td>-0.0580</td>\n","      <td>-0.3608</td>\n","      <td>0.2197</td>\n","      <td>-0.7101</td>\n","      <td>1.3430</td>\n","      <td>...</td>\n","      <td>-0.7458</td>\n","      <td>0.0458</td>\n","      <td>-0.3644</td>\n","      <td>-1.8180</td>\n","      <td>-0.0358</td>\n","      <td>-0.7925</td>\n","      <td>-0.2693</td>\n","      <td>-0.0938</td>\n","      <td>-0.1833</td>\n","      <td>-0.7402</td>\n","      <td>-1.4090</td>\n","      <td>0.1987</td>\n","      <td>0.0460</td>\n","      <td>-1.3520</td>\n","      <td>-0.3445</td>\n","      <td>-0.0909</td>\n","      <td>-0.6337</td>\n","      <td>-0.5788</td>\n","      <td>-0.7885</td>\n","      <td>0.0996</td>\n","      <td>-1.9480</td>\n","      <td>-1.2720</td>\n","      <td>-0.7223</td>\n","      <td>-0.5838</td>\n","      <td>-1.3620</td>\n","      <td>-0.7671</td>\n","      <td>0.4881</td>\n","      <td>0.5913</td>\n","      <td>-0.4333</td>\n","      <td>0.1234</td>\n","      <td>-0.1190</td>\n","      <td>-0.1852</td>\n","      <td>-1.0310</td>\n","      <td>-1.3670</td>\n","      <td>-0.3690</td>\n","      <td>-0.5382</td>\n","      <td>0.0359</td>\n","      <td>-0.4764</td>\n","      <td>-1.3810</td>\n","      <td>-0.7300</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>id_002429b5b</td>\n","      <td>ctl_vehicle</td>\n","      <td>24</td>\n","      <td>D1</td>\n","      <td>0.1852</td>\n","      <td>-0.1404</td>\n","      <td>-0.3911</td>\n","      <td>0.1310</td>\n","      <td>-1.4380</td>\n","      <td>0.2455</td>\n","      <td>-0.3390</td>\n","      <td>-0.3206</td>\n","      <td>0.6944</td>\n","      <td>0.5837</td>\n","      <td>-0.0553</td>\n","      <td>-0.6222</td>\n","      <td>2.5430</td>\n","      <td>-0.7857</td>\n","      <td>0.8163</td>\n","      <td>-0.0495</td>\n","      <td>0.1806</td>\n","      <td>1.0290</td>\n","      <td>-0.5204</td>\n","      <td>-1.1070</td>\n","      <td>0.7365</td>\n","      <td>-0.3835</td>\n","      <td>-0.5771</td>\n","      <td>0.0523</td>\n","      <td>-0.2690</td>\n","      <td>0.1674</td>\n","      <td>0.6010</td>\n","      <td>-0.6660</td>\n","      <td>0.0276</td>\n","      <td>0.0924</td>\n","      <td>0.2785</td>\n","      <td>-0.3943</td>\n","      <td>-0.4602</td>\n","      <td>-0.0673</td>\n","      <td>-1.3420</td>\n","      <td>0.3127</td>\n","      <td>...</td>\n","      <td>0.4369</td>\n","      <td>-1.4960</td>\n","      <td>1.2390</td>\n","      <td>-1.2220</td>\n","      <td>0.6624</td>\n","      <td>-0.7336</td>\n","      <td>-0.5248</td>\n","      <td>0.0727</td>\n","      <td>0.1455</td>\n","      <td>0.5364</td>\n","      <td>-0.0823</td>\n","      <td>0.5734</td>\n","      <td>0.4876</td>\n","      <td>0.7088</td>\n","      <td>1.0750</td>\n","      <td>0.4689</td>\n","      <td>1.0870</td>\n","      <td>-0.5036</td>\n","      <td>-0.3451</td>\n","      <td>0.5087</td>\n","      <td>1.1100</td>\n","      <td>0.7886</td>\n","      <td>0.2093</td>\n","      <td>-0.4617</td>\n","      <td>1.4870</td>\n","      <td>0.1985</td>\n","      <td>1.1750</td>\n","      <td>-0.5693</td>\n","      <td>0.5062</td>\n","      <td>-0.1925</td>\n","      <td>-0.2261</td>\n","      <td>0.3370</td>\n","      <td>-1.3840</td>\n","      <td>0.8604</td>\n","      <td>-1.9530</td>\n","      <td>-1.0140</td>\n","      <td>0.8662</td>\n","      <td>1.0160</td>\n","      <td>0.4924</td>\n","      <td>-0.1942</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>id_00276f245</td>\n","      <td>trt_cp</td>\n","      <td>24</td>\n","      <td>D2</td>\n","      <td>0.4828</td>\n","      <td>0.1955</td>\n","      <td>0.3825</td>\n","      <td>0.4244</td>\n","      <td>-0.5855</td>\n","      <td>-1.2020</td>\n","      <td>0.5998</td>\n","      <td>-0.1799</td>\n","      <td>0.9365</td>\n","      <td>0.2942</td>\n","      <td>1.3990</td>\n","      <td>-1.1080</td>\n","      <td>1.1610</td>\n","      <td>-0.8134</td>\n","      <td>0.0743</td>\n","      <td>-0.3096</td>\n","      <td>-0.9956</td>\n","      <td>1.7100</td>\n","      <td>1.2200</td>\n","      <td>-0.1258</td>\n","      <td>-0.5900</td>\n","      <td>-0.7956</td>\n","      <td>-0.6765</td>\n","      <td>0.0000</td>\n","      <td>-0.8820</td>\n","      <td>0.7609</td>\n","      <td>-0.0882</td>\n","      <td>0.5217</td>\n","      <td>0.9587</td>\n","      <td>-0.4764</td>\n","      <td>0.2690</td>\n","      <td>0.4753</td>\n","      <td>0.0196</td>\n","      <td>0.2775</td>\n","      <td>-0.7142</td>\n","      <td>0.7079</td>\n","      <td>...</td>\n","      <td>-1.2190</td>\n","      <td>-0.5564</td>\n","      <td>-0.2831</td>\n","      <td>0.5902</td>\n","      <td>-0.2881</td>\n","      <td>-0.0013</td>\n","      <td>-0.4036</td>\n","      <td>-0.4076</td>\n","      <td>-0.5593</td>\n","      <td>-0.5505</td>\n","      <td>-0.4806</td>\n","      <td>-0.0846</td>\n","      <td>0.5884</td>\n","      <td>0.2310</td>\n","      <td>0.3956</td>\n","      <td>0.6495</td>\n","      <td>-0.2511</td>\n","      <td>-0.2207</td>\n","      <td>0.1274</td>\n","      <td>-0.5378</td>\n","      <td>0.5649</td>\n","      <td>-0.1231</td>\n","      <td>0.1586</td>\n","      <td>-0.4260</td>\n","      <td>-0.6815</td>\n","      <td>-0.4753</td>\n","      <td>0.2611</td>\n","      <td>-1.1780</td>\n","      <td>0.1909</td>\n","      <td>-1.2320</td>\n","      <td>0.1260</td>\n","      <td>0.1570</td>\n","      <td>-0.1784</td>\n","      <td>-1.1200</td>\n","      <td>-0.4325</td>\n","      <td>-0.9005</td>\n","      <td>0.8131</td>\n","      <td>-0.1305</td>\n","      <td>0.5645</td>\n","      <td>-0.5809</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>id_0027f1083</td>\n","      <td>trt_cp</td>\n","      <td>48</td>\n","      <td>D1</td>\n","      <td>-0.3979</td>\n","      <td>-1.2680</td>\n","      <td>1.9130</td>\n","      <td>0.2057</td>\n","      <td>-0.5864</td>\n","      <td>-0.0166</td>\n","      <td>0.5128</td>\n","      <td>0.6365</td>\n","      <td>0.2611</td>\n","      <td>-1.1120</td>\n","      <td>-0.1747</td>\n","      <td>-0.2053</td>\n","      <td>-0.0006</td>\n","      <td>0.0254</td>\n","      <td>0.1784</td>\n","      <td>-0.1314</td>\n","      <td>1.0700</td>\n","      <td>0.0477</td>\n","      <td>-0.0756</td>\n","      <td>0.1287</td>\n","      <td>0.1403</td>\n","      <td>0.7609</td>\n","      <td>-0.1095</td>\n","      <td>0.0000</td>\n","      <td>-1.0950</td>\n","      <td>-0.0780</td>\n","      <td>0.9048</td>\n","      <td>-0.3007</td>\n","      <td>0.4351</td>\n","      <td>-0.1558</td>\n","      <td>0.2101</td>\n","      <td>0.4850</td>\n","      <td>0.1139</td>\n","      <td>0.6745</td>\n","      <td>-0.9101</td>\n","      <td>-0.6690</td>\n","      <td>...</td>\n","      <td>0.6412</td>\n","      <td>0.2123</td>\n","      <td>0.0646</td>\n","      <td>0.5924</td>\n","      <td>0.5284</td>\n","      <td>-0.4421</td>\n","      <td>0.6547</td>\n","      <td>-0.8098</td>\n","      <td>0.9812</td>\n","      <td>-0.0349</td>\n","      <td>0.7816</td>\n","      <td>1.1690</td>\n","      <td>0.4140</td>\n","      <td>1.0250</td>\n","      <td>0.8642</td>\n","      <td>0.5993</td>\n","      <td>0.2091</td>\n","      <td>-0.1521</td>\n","      <td>1.5610</td>\n","      <td>-0.6770</td>\n","      <td>0.3600</td>\n","      <td>-0.1436</td>\n","      <td>-0.0949</td>\n","      <td>1.0640</td>\n","      <td>0.8321</td>\n","      <td>0.9727</td>\n","      <td>-0.5567</td>\n","      <td>0.2240</td>\n","      <td>0.8949</td>\n","      <td>0.8668</td>\n","      <td>0.4965</td>\n","      <td>0.7578</td>\n","      <td>-0.1580</td>\n","      <td>1.0510</td>\n","      <td>0.5742</td>\n","      <td>1.0900</td>\n","      <td>-0.2962</td>\n","      <td>-0.5313</td>\n","      <td>0.9931</td>\n","      <td>1.8380</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 876 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e5be4907-b8a6-4226-90cb-1a58bb602f32')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-e5be4907-b8a6-4226-90cb-1a58bb602f32 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-e5be4907-b8a6-4226-90cb-1a58bb602f32');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["         sig_id      cp_type  cp_time cp_dose  ...    c-96    c-97    c-98    c-99\n","0  id_0004d9e33       trt_cp       24      D1  ...  0.0210 -0.0502  0.1510 -0.7750\n","1  id_001897cda       trt_cp       72      D1  ...  0.0359 -0.4764 -1.3810 -0.7300\n","2  id_002429b5b  ctl_vehicle       24      D1  ...  0.8662  1.0160  0.4924 -0.1942\n","3  id_00276f245       trt_cp       24      D2  ...  0.8131 -0.1305  0.5645 -0.5809\n","4  id_0027f1083       trt_cp       48      D1  ... -0.2962 -0.5313  0.9931  1.8380\n","\n","[5 rows x 876 columns]"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"bs8oRpwz-n6F"},"source":["### Basic Setup and helpers\n"]},{"cell_type":"code","metadata":{"id":"u4PcUn_N-q0M"},"source":["SEED = 1234\n","EPOCHS = 4\n","BATCH_SIZE = 16\n","FOLDS = 3\n","REPEATS = 2\n","LR = 0.05\n","N_TARGETS = len(target_cols)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X8n5JV7E-s3Q"},"source":["def seed_everything(seed):\n","  np.random.seed(seed)\n","  os.environ['PYTHONHASHSEED'] = str(seed)\n","  tf.random.set_seed(seed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Rh9sWq_-0O7"},"source":["def multi_log_loss(y_true, y_pred):\n","  losses = []\n","  for col in y_true.columns:\n","    losses.append(log_loss(y_true.loc[:, col], y_pred.loc[:, col]))\n","  return np.mean(losses)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sM4htav5--Hu"},"source":["### Encode Categoricals to Binary"]},{"cell_type":"code","metadata":{"id":"Ng7QQ6et_BTW"},"source":["def preprocess_df(data):\n","  data['cp_type'] = (data['cp_type'] == 'trt_cp').astype(int)\n","  data['cp_dose'] = (data['cp_dose'] == 'D2').astype(int)\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-4f05VHV_Jkt"},"source":["x_train = preprocess_df(train_df.drop(columns=\"sig_id\"))\n","x_test =preprocess_df(test_df.drop(columns=\"sig_id\"))\n","y_train = train_target_df.drop(columns=\"sig_id\")\n","N_FEATURES = x_train.shape[1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UwbFCcwz_Lkl"},"source":["### Define Model Architecture\n"]},{"cell_type":"code","metadata":{"id":"0U5ukdyi_NES"},"source":["def create_model():\n","  model = tf.keras.Sequential([\n","                               tf.keras.layers.Input(N_FEATURES),\n","                               tf.keras.layers.BatchNormalization(),\n","                               tf.keras.layers.Dropout(0.2),\n","                               tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation = 'relu')\n","                               ),\n","                               tf.keras.layers.BatchNormalization(),\n","                               tf.keras.layers.Dropout(0.5),\n","                               tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation = 'relu')\n","                               ),\n","                               tf.keras.layers.BatchNormalization(),\n","                               tf.keras.layers.Dropout(0.4),\n","                               tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation = 'relu')\n","                               ),\n","                               tf.keras.layers.BatchNormalization(),\n","                               tf.keras.layers.Dropout(0.5),\n","                               tfa.layers.WeightNormalization(tf.keras.layers.Dense(N_TARGETS, activation = 'sigmoid'))\n","  ])\n","  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = LR), loss = 'binary_crossentropy', metrics=['accuracy'])\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wDn9dEDvALZM"},"source":["### Main CV and Model Training Function\n"]},{"cell_type":"code","metadata":{"id":"751bGupTARYZ"},"source":["def build_train(resume_models = None, repeat_number = 0, folds = 3, skip_folds = 0):\n","  models = []\n","  oof_preds = y_train.copy()\n","\n","  kfold = KFold(folds, shuffle = True)\n","  for fold, (train_ind, val_ind) in enumerate(kfold.split(x_train)):\n","    print('\\n')\n","    print('-'*50)\n","    print(f'Training fold {fold + 1}')\n","\n","    cb_lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'binary_crossentropy',\n","                                                          factor = 0.4,\n","                                                          patience = 2,\n","                                                          verbose = 1,\n","                                                          min_delta = 0.0001,\n","                                                          mode = 'auto')\n","    checkpoint_path = f'repeat:{repeat_number}_Fold:{fold}.hdf5'\n","    cb_checkpt = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, \n","                                                    monitor = 'val_loss',\n","                                                    verbose = 0,\n","                                                    save_best_only = True,\n","                                                    save_weights_only = True,\n","                                                    mode = 'min')\n","    model = create_model()\n","    model.fit(x_train.values[train_ind],\n","              y_train.values[train_ind],\n","              validation_data = (x_train.values[val_ind], y_train.values[val_ind]),\n","              callbacks = [cb_lr_schedule, cb_checkpt],\n","              epochs = EPOCHS, batch_size = BATCH_SIZE, verbose = 2\n","              )\n","    model.load_weights(checkpoint_path)\n","    oof_preds.loc[val_ind, :] = model.predict(x_train.values[val_ind])\n","    models.append(model)\n","\n","  return models, oof_preds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qQcwjhVdBL79","executionInfo":{"status":"ok","timestamp":1642414409703,"user_tz":-480,"elapsed":2722837,"user":{"displayName":"Indrik Wijaya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00208067985282844199"}},"outputId":"831a899f-c281-44a5-b067-60e081dd4903"},"source":["models = []\n","oof_preds = []\n","seed_everything(SEED)\n","for i in range(REPEATS):\n","  m, oof = build_train(repeat_number=i, folds = FOLDS)\n","  models = models + m\n","  oof_preds.append(oof)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","--------------------------------------------------\n","Training fold 1\n","Epoch 1/4\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `binary_crossentropy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n","417/417 - 109s - loss: 0.0303 - accuracy: 0.0332 - val_loss: 0.0210 - val_accuracy: 0.0678 - lr: 0.0500 - 109s/epoch - 262ms/step\n","Epoch 2/4\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `binary_crossentropy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n","417/417 - 104s - loss: 0.0209 - accuracy: 0.0432 - val_loss: 0.0192 - val_accuracy: 0.0567 - lr: 0.0500 - 104s/epoch - 251ms/step\n","Epoch 3/4\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `binary_crossentropy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n","417/417 - 107s - loss: 0.0194 - accuracy: 0.0636 - val_loss: 0.0186 - val_accuracy: 0.0699 - lr: 0.0500 - 107s/epoch - 256ms/step\n","Epoch 4/4\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `binary_crossentropy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n","417/417 - 106s - loss: 0.0186 - accuracy: 0.0722 - val_loss: 0.0181 - val_accuracy: 0.0840 - lr: 0.0500 - 106s/epoch - 253ms/step\n","\n","\n","--------------------------------------------------\n","Training fold 2\n","Epoch 1/4\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `binary_crossentropy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n","417/417 - 106s - loss: 0.0293 - accuracy: 0.0274 - val_loss: 0.0217 - val_accuracy: 0.0288 - lr: 0.0500 - 106s/epoch - 255ms/step\n","Epoch 2/4\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `binary_crossentropy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n","417/417 - 105s - loss: 0.0207 - accuracy: 0.0391 - val_loss: 0.0195 - val_accuracy: 0.0555 - lr: 0.0500 - 105s/epoch - 252ms/step\n","Epoch 3/4\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `binary_crossentropy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n","417/417 - 104s - loss: 0.0192 - accuracy: 0.0613 - val_loss: 0.0190 - val_accuracy: 0.1026 - lr: 0.0500 - 104s/epoch - 249ms/step\n","Epoch 4/4\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `binary_crossentropy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n","417/417 - 105s - loss: 0.0183 - accuracy: 0.0774 - val_loss: 0.0191 - val_accuracy: 0.1083 - lr: 0.0500 - 105s/epoch - 251ms/step\n","\n","\n","--------------------------------------------------\n","Training fold 3\n","Epoch 1/4\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `binary_crossentropy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n","417/417 - 104s - loss: 0.0310 - accuracy: 0.0333 - val_loss: 0.0204 - val_accuracy: 0.0360 - lr: 0.0500 - 104s/epoch - 250ms/step\n","Epoch 2/4\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `binary_crossentropy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n","417/417 - 100s - loss: 0.0204 - accuracy: 0.0577 - val_loss: 0.0195 - val_accuracy: 0.0837 - lr: 0.0500 - 100s/epoch - 240ms/step\n","Epoch 3/4\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `binary_crossentropy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n","417/417 - 103s - loss: 0.0190 - accuracy: 0.0687 - val_loss: 0.0185 - val_accuracy: 0.0774 - lr: 0.0500 - 103s/epoch - 246ms/step\n","Epoch 4/4\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `binary_crossentropy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n","417/417 - 102s - loss: 0.0184 - accuracy: 0.0847 - val_loss: 0.0181 - val_accuracy: 0.0780 - lr: 0.0500 - 102s/epoch - 245ms/step\n","\n","\n","--------------------------------------------------\n","Training fold 1\n","Epoch 1/4\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `binary_crossentropy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n","417/417 - 120s - loss: 0.0318 - accuracy: 0.0312 - val_loss: 0.0212 - val_accuracy: 0.0357 - lr: 0.0500 - 120s/epoch - 288ms/step\n","Epoch 2/4\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `binary_crossentropy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n","417/417 - 117s - loss: 0.0204 - accuracy: 0.0515 - val_loss: 0.0202 - val_accuracy: 0.0636 - lr: 0.0500 - 117s/epoch - 281ms/step\n","Epoch 3/4\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `binary_crossentropy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n","417/417 - 115s - loss: 0.0189 - accuracy: 0.0738 - val_loss: 0.0188 - val_accuracy: 0.0948 - lr: 0.0500 - 115s/epoch - 276ms/step\n","Epoch 4/4\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `binary_crossentropy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n","417/417 - 114s - loss: 0.0179 - accuracy: 0.0891 - val_loss: 0.0183 - val_accuracy: 0.0843 - lr: 0.0500 - 114s/epoch - 273ms/step\n","\n","\n","--------------------------------------------------\n","Training fold 2\n","Epoch 1/4\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `binary_crossentropy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n","417/417 - 108s - loss: 0.0307 - accuracy: 0.0297 - val_loss: 0.0217 - val_accuracy: 0.0504 - lr: 0.0500 - 108s/epoch - 260ms/step\n","Epoch 2/4\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `binary_crossentropy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n","417/417 - 102s - loss: 0.0209 - accuracy: 0.0381 - val_loss: 0.0200 - val_accuracy: 0.0531 - lr: 0.0500 - 102s/epoch - 244ms/step\n","Epoch 3/4\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `binary_crossentropy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n","417/417 - 102s - loss: 0.0196 - accuracy: 0.0531 - val_loss: 0.0184 - val_accuracy: 0.0717 - lr: 0.0500 - 102s/epoch - 244ms/step\n","Epoch 4/4\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `binary_crossentropy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n","417/417 - 102s - loss: 0.0187 - accuracy: 0.0660 - val_loss: 0.0177 - val_accuracy: 0.0816 - lr: 0.0500 - 102s/epoch - 244ms/step\n","\n","\n","--------------------------------------------------\n","Training fold 3\n","Epoch 1/4\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `binary_crossentropy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n","417/417 - 110s - loss: 0.0299 - accuracy: 0.0283 - val_loss: 0.0208 - val_accuracy: 0.0279 - lr: 0.0500 - 110s/epoch - 264ms/step\n","Epoch 2/4\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `binary_crossentropy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n","417/417 - 108s - loss: 0.0208 - accuracy: 0.0393 - val_loss: 0.0192 - val_accuracy: 0.0480 - lr: 0.0500 - 108s/epoch - 259ms/step\n","Epoch 3/4\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `binary_crossentropy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n","417/417 - 110s - loss: 0.0195 - accuracy: 0.0594 - val_loss: 0.0184 - val_accuracy: 0.0897 - lr: 0.0500 - 110s/epoch - 263ms/step\n","Epoch 4/4\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `binary_crossentropy` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n","417/417 - 106s - loss: 0.0187 - accuracy: 0.0732 - val_loss: 0.0184 - val_accuracy: 0.0723 - lr: 0.0500 - 106s/epoch - 255ms/step\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0u7pr_mlBVOn","executionInfo":{"status":"ok","timestamp":1642414415568,"user_tz":-480,"elapsed":5876,"user":{"displayName":"Indrik Wijaya","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00208067985282844199"}},"outputId":"d4fa2a19-3f74-4109-c379-2ba22c6136e8"},"source":["mean_oof_preds = y_train.copy()\n","mean_oof_preds.loc[:, target_cols] = 0\n","for i, p in enumerate(oof_preds):\n","    print(f\"Repeat {i + 1} OOF Log Loss: {multi_log_loss(y_train, p)}\")\n","    mean_oof_preds.loc[:, target_cols] += p[target_cols]\n","\n","mean_oof_preds.loc[:, target_cols] /= len(oof_preds)\n","print(f\"Mean OOF Log Loss: {multi_log_loss(y_train, mean_oof_preds)}\")\n","# mean_oof_preds.loc[x_train['cp_type'] == 0, target_cols] = 0\n","# print(f\"Mean OOF Log Loss (ctl adjusted): {multi_log_loss(y_train, mean_oof_preds)}\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Repeat 1 OOF Log Loss: 0.007718477756900262\n","Repeat 2 OOF Log Loss: 0.0076175487503934965\n","Mean OOF Log Loss: 0.007341890669705138\n"]}]},{"cell_type":"code","metadata":{"id":"rKBr4vHYFUX8"},"source":["test_preds = sub.copy()\n","test_preds[target_cols] = 0\n","for model in models:\n","    test_preds.loc[:,target_cols] += model.predict(x_test)\n","test_preds.loc[:,target_cols] /= len(models)\n","test_preds.loc[x_test['cp_type'] == 0, target_cols] = 0\n","test_preds.to_csv('submission.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zRb8Cx2EJa7B"},"source":[""],"execution_count":null,"outputs":[]}]}